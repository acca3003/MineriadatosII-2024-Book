% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=28mm,bottom=30mm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{makeidx}
\makeindex

% Poner el texto adecuado por delante
\renewcommand{\figurename}{Imagen}
\renewcommand{\tablename}{Tabla}

% Formatear el Capitulo, secci√≥n y subsecci√≥n
\setkomafont{chapter}{\normalfont\huge\sffamily\bfseries\color{blue}}
\addtokomafont{section}{\color{blue}}
\addtokomafont{subsection}{\color{blue}}

% Para las cabeceras y pies (sin la l√≠neas en las cabeceras, usaremos tcolorbox)
\usepackage{scrlayer-scrpage}
\pagestyle{scrheadings}

% Misma cabecera y pie en p√°gina de Capiutlo
\renewcommand*{\chapterpagestyle}{scrheadings}

% Para que luego se ponga correctamente el nombre del capitulo
\automark[chapter]{chapter}

% Para personalizar los bloques de c√≥digo
\usepackage{xcolor}
\usepackage{listings}
\lstset{breaklines=true}
%\lstset{language=[python]}
\lstset{basicstyle=\small\ttfamily}
\lstset{extendedchars=true}
\lstset{tabsize=2}
\lstset{columns=fixed}
\lstset{showstringspaces=false}
\lstset{frame=single}
%\lstset{frameround=tttt}
%\lstset{framesep=4pt}
\lstset{numbers=left}
\lstset{numberstyle=\tiny\ttfamily}
\lstset{postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}}
\KOMAoption{captions}{tableheading}
\titlehead{\center{\includegraphics[]{imagenes/Licencia.png} \includegraphics[]{imagenes/portada.png} }}
\author{First Author}
\author{Second Author}
\author{Third Author}
\author{Supervisor Author}

% Colocar elementos en Cabecera y pie

\cehead[]{
        \begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
          Miner√≠a de datos II   \hfill          \leftmark
        \end{tcolorbox}} 

\cohead[]{              
        \begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
          Miner√≠a de datos II   \hfill          \leftmark
        \end{tcolorbox}} 
\cofoot[]{              
        \begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
          \hfill          -\thepage -
        \end{tcolorbox}}   
\cefoot[]{              
        \begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
          \hfill          -\thepage -
        \end{tcolorbox}}  
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={M√≥dulo 8. Miner√≠a de Datos II},
  pdfauthor={Pablo S√°nchez Cabrera; √Ångel Rodr√≠guez Chicote; Alfonso Carabantes √Ålamo},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{M√≥dulo 8. Miner√≠a de Datos II}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Curso Modular Big Data y Data Science Aplicados a la Econom√≠a
y a la Administraci√≥n y Direcci√≥n de Empresas}
\author{Pablo S√°nchez Cabrera \and √Ångel Rodr√≠guez Chicote \and Alfonso
Carabantes √Ålamo}
\date{2024-07-01}

\begin{document}
\maketitle
\renewcommand{\figurename}{Imagen}
\KOMAoptions{footsepline=true,footbotline=true}

\renewcommand*\contentsname{√çndice}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{introducciuxf3n}{%
\chapter{Introducci√≥n}\label{introducciuxf3n}}

Anotaci√≥n\footnote{Anotaci√≥n de prueba} Diagrama Conceptual M√≥dulo 8:
Miner√≠a de Datos II

Referecnia bibliografica (Knuth 1984)

Referencia gr√°fico Figure~\ref{fig-diagrama-conceptual} Otra referencia
a gr√°fico \ref{fig-diagrama-conceptual}

\begin{tcolorbox}[enhanced jigsaw, colback=white, toprule=.15mm, breakable, toptitle=1mm, titlerule=0mm, coltitle=black, left=2mm, opacityback=0, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, bottomrule=.15mm, leftrule=.75mm]

Note that there are five types of callouts, including: \texttt{note},
\texttt{warning}, \texttt{important}, \texttt{tip}, and
\texttt{caution}.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, toprule=.15mm, breakable, toptitle=1mm, titlerule=0mm, coltitle=black, left=2mm, opacityback=0, colbacktitle=quarto-callout-warning-color!10!white, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, rightrule=.15mm, bottomtitle=1mm, bottomrule=.15mm, leftrule=.75mm]

Atenci√≥n!!!

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, toprule=.15mm, breakable, toptitle=1mm, titlerule=0mm, coltitle=black, left=2mm, opacityback=0, colbacktitle=quarto-callout-important-color!10!white, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, rightrule=.15mm, bottomtitle=1mm, bottomrule=.15mm, leftrule=.75mm]

Importante

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, toprule=.15mm, breakable, toptitle=1mm, titlerule=0mm, coltitle=black, left=2mm, opacityback=0, colbacktitle=quarto-callout-tip-color!10!white, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip with Title}, rightrule=.15mm, bottomtitle=1mm, bottomrule=.15mm, leftrule=.75mm]

This is an example of a callout with a title.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, toprule=.15mm, breakable, toptitle=1mm, titlerule=0mm, coltitle=black, left=2mm, opacityback=0, colbacktitle=quarto-callout-caution-color!10!white, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-caution-color-frame, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Expand To Learn About Collapse}, rightrule=.15mm, bottomtitle=1mm, bottomrule=.15mm, leftrule=.75mm]

This is an example of a `folded' caution callout that can be expanded by
the user. You can use \texttt{collapse="true"} to collapse it by default
or \texttt{collapse="false"} to make a collapsible callout that is
expanded by default.

\end{tcolorbox}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{imagenes/diagrama_conceptual.png}

}

\caption{\label{fig-diagrama-conceptual}Diagrama Conceptual}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{deep-learning}{%
\chapter{Deep Learning}\label{deep-learning}}

\hypertarget{introducciuxf3n-1}{%
\section{Introducci√≥n}\label{introducciuxf3n-1}}

El \textbf{deep learning o aprendizaje profundo} es un concepto amplio,
un t√©rmino de moda que atiende a una realidad que cada d√≠a est√° m√°s
presente entre nosotros, y que no tiene una √∫nica definici√≥n que se
pueda considerar veraz, aunque podemos generalizar la definici√≥n
afirmando que surge de la idea de imitar el cerebro a partir de la
utilizaci√≥n de hardware y software, para crear inteligencia artificial.
Este intento de imitar al cerebro se ha plasmado en lo que se conoce
como redes neuronales artificiales (RNA) que est√°n dotadas de una
capacidad de abstracci√≥n jer√°rquica: representan los datos de entrada en
varios ``niveles'', a trav√©s de arquitecturas en varias capas, cada capa
aprende patrones m√°s complejos seg√∫n la profundidad de √©sta, para
conseguir informaci√≥n √∫til para el aprendizaje.

Podemos afirmar que el principio general del funcionamiento de una
arquitectura profunda es guiar el entrenamiento de las capas intermedias
utilizando aprendizaje no supervisado. Algunos de los prototipos de
arquitecturas y/o algoritmos desarrollados se basan en otras
arquitecturas ``m√°s simples'', modific√°ndolas y obteniendo una
arquitectura ``m√°s profunda'', lo que quiere decir que en sus capas y
neuronas se separan las caracter√≠sticas de un conjunto de datos de
entrada de una forma jerarquizada.

Otra definici√≥n dada del aprendizaje profundo es que es un subconjunto
de los algoritmos usados en machine learning, y se caracterizan por
tener una \textbf{arquitectura en capas} donde cada capa aprende
patrones m√°s complejos seg√∫n la profundidad de √©sta. En este sentido,
existen ya muchos prototipos de redes neuronales que tienen esta
caracter√≠stica, por ejemplo, las ya muy conocidas redes convolucionales
o las redes recurrentes. El Perceptron Multicapa de una sola capa
explicado en el m√≥dulo 5 no es considerado un m√©todo de aprendizaje
profundo. El Deep Learning es una t√©cnica reciente donde a√∫n se sigue
investigando y desarroll√°ndose y que surgi√≥ a partir del a√±o 2006, a√±o
en el que se considera fue retomada su investigaci√≥n. El principal
problema por el que se hab√≠a ralentizado su progreso es que el
entrenamiento basado en el m√©todo del gradiente para redes neuronales
profundas supervisadas, seg√∫n muchos investigadores, se estancaba en lo
que se llama m√≠nimo local aparente, lo que significa que una red con m√°s
capas ocultas, es decir una red neuronal m√°s profunda, obtiene peores
resultados que una red con s√≥lo una capa oculta. Los investigadores
abordaron este tema planteado a trav√©s de diferentes √≥pticas y se
descubre que se obtienen resultados m√°s precisos si cada capa de la red
es entrenada a trav√©s de un algoritmo de preentrenamiento no supervisado
(el m√©todo del gradiente requiere que sea supervisado) Para el
entrenamiento de las capas, diversos autores sugieren utilizar
autoencoders o m√°quinas de Boltzmann y, una vez se hayan preentrenado
las diferentes capas, se puede aplicar un criterio supervisado. En estos
√∫ltimos a√±os se ha producido un r√°pido crecimiento en la cantidad de
arquitecturas y/o algoritmos de entrenamiento en una RNA, incluso,
variantes de ellos, las cuales siguen el concepto planteado
anteriormente.

\hypertarget{revisiuxf3n-de-las-redes-neuronales}{%
\section{Revisi√≥n de las Redes
Neuronales}\label{revisiuxf3n-de-las-redes-neuronales}}

Vamos a hacer una revisi√≥n de las redes neuronales para posteriormente
poder abordar los diferentes tipos de redes neuronales que se utilizan
en Deep Learning. Algunos de los avances m√°s recientes en varios de los
diferentes componentes que forman parte de las redes neuronales est√°n
recopilados en (Gu et al.~2017) Las redes neuronales artificiales tienen
sus or√≠genes en el Perceptr√≥n, que fue el modelo creado por Frank
Rosenblatt en 1957 y basado en los trabajos que previamente hab√≠an
realizado Warren McCullon (neurofisi√≥logo) y Walter Pitts (matem√°tico).
El Perceptr√≥n est√° construido por una neurona artificial cuyas entradas
y salida pueden ser datos num√©ricos, no como pasaba con la neurona de
McCulloch y Pitts (eran s√≥lo datos l√≥gicos). Las neuronas pueden tener
pesos y adem√°s se le aplica una funci√≥n de activaci√≥n Sigmoid (a
diferencia de la usada anteriormente al Paso binario). En esta neurona
nos encontramos que se realizan los siguientes c√°lculos:
\[ z = \sum_{i=1}^{n}w_ix_i+b_i\] \[\hat{y} = \delta (z)\]

donde representan los datos num√©ricos de entrada, son los pesos, es el
sesgo (bias), es la funci√≥n de activaci√≥n y finalmente es el dato de
salida. El modelo de perceptr√≥n es el m√°s simple, en el que hay una sola
capa oculta con una √∫nica neurona. El siguiente paso nos lleva al
Perceptr√≥n Multicapa donde ya pasamos a tener m√°s de una capa oculta, y
adem√°s podemos tener m√∫ltiples neuronas en cada capa oculta. Cuando
todas las neuronas de una capa est√°n interconectadas con todas las de la
siguiente capa estamos ante una red neuronal densamente conectada. A lo
largo de las siguientes secciones nos encontraremos con redes en las que
no todas las neuronas de una capa se conectan con todas de la siguiente.
Veamos como describir√≠amos ahora los resultados de las capas donde
representan los datos de la neurona en la capa ( siendo los valores de
entrada), son los pesos en la capa , es el sesgo (bias) en la capa , es
la funci√≥n de activaci√≥n en la capa (puede que cada capa tenga una
funci√≥n de activaci√≥n diferente), es el n√∫mero de neurona de la capa
anterior que conectan con la y finalmente es el dato de salida de la
capa . Es decir, en cada capa para calcular el nuevo valor necesitamos
usar los valores de la capa anterior.

\textbf{Aplicaciones de las Redes Neuronales}

Cada d√≠a las redes neuronales est√°n m√°s presentes en diferentes campos y
ayudan a resolver una gran variedad de problemas. Podr√≠amos pensar que
de forma m√°s b√°sica una red neuronal nos puede ayudar a resolver
problemas de regresi√≥n y clasificaci√≥n, es decir, podr√≠amos considerarlo
como otro modelo m√°s de los existentes que a partir de unos datos de
entrada somos capaces de obtener o un dato num√©rico (o varios) para
hacer una regresi√≥n (calcular en precio de una vivienda en funci√≥n de
diferentes valores de la misma) o que somos capaces de conseguir que en
funci√≥n de los datos de entrada nos deje clasificada una muestra
(decidir si conceder o no una hipoteca en funci√≥n de diferentes datos
del cliente). Si los datos de entrada son im√°genes podr√≠amos estar
usando las redes neuronales como una forma de identificar esa imagen: ‚Ä¢
Identificando que tipo de animal es ‚Ä¢ Identificando que se√±al de tr√°fico
es ‚Ä¢ Identificando que tipo de fruta es ‚Ä¢ Identificando que una imagen
es de exterior o interior de una casa ‚Ä¢ Identificando que es una cara de
una persona ‚Ä¢ Identificando que una imagen radiogr√°fica represente un
tumor maligno ‚Ä¢ Identificando que haya texto en una imagen Luego
podr√≠amos pasar a revolver problemas m√°s complejos combinando las
capacidades anteriores: ‚Ä¢ Detectar los diferentes objetos y personas que
se encuentran en una imagen ‚Ä¢ Etiquedado de escenas (aula con alumnos,
partido de futbol, etc\ldots) Despu√©s podr√≠amos dar el paso al video que
lo podr√≠amos considerar como una secuencia de im√°genes: ‚Ä¢ Contar el
n√∫mero de personas que entran y salen de una habitaci√≥n ‚Ä¢ Reconocer que
es una carretera ‚Ä¢ Identificar las se√±ales de tr√°fico ‚Ä¢ Detectar si
alguien lleva un arma ‚Ä¢ Seguimiento de objetos ‚Ä¢ Detecci√≥n de
estado/actitud de una persona ‚Ä¢ Reconocimiento de acciones (interpretar
lenguaje de signos, interpretar lenguaje de banderas) ‚Ä¢ Veh√≠culos
inteligentes Si los datos de entrada son secuencias de texto ‚Ä¢ Sistemas
de traducci√≥n ‚Ä¢ Chatbots (resoluci√≥n de preguntas a usuarios) ‚Ä¢
Conversi√≥n de texto a audio Si los datos de entrada son audios ‚Ä¢
Sistemas de traducci√≥n ‚Ä¢ Altavoces inteligentes ‚Ä¢ Conversi√≥n de audio a
texto

A continuaci√≥n, pasamos a revisar diferentes elementos de las redes
neuronales que suelen ser comunes a todos los tipos de redes neuronales.

\textbf{Datos}

Cuando se trabaja con redes neuronales necesitamos representar los
valores de las variables de entrada en forma num√©rica. En una red
neuronal todos los datos son siempre num√©ricos. Esto significa que todas
aquellas variables que sean categ√≥ricas necesitamos convertirlas en
num√©ricas. Adem√°s, es muy conveniente normalizar los datos para poder
trabajar con valores entre 0 y 1, que van a ayudar a que sea m√°s f√°cil
que se pueda converger a la soluci√≥n. Es importante que los datos se√°n
n√∫meros en coma flotante, sobre todo si se van a trabajar con GPUs
(Graphics Process Units), ya que permitir√°n hacer un mejor uso de los
multiples cores que les permiten operar en coma flotante de forma
paralela. Actualmente, hay toda una serie de mejoras en las GPUs que
permite aumentar el rendimiento de las redes neuronales como son el uso
de operaciones en FP16 (Floating Point de 16 bits en lugar de 32) de
forma que pueden hacer dos operaciones de forma simult√°nea (el formato
est√°ndar es FP32) y adem√°s con la reducci√≥n de memoria (punto muy
importante) al meter en los 32 bits 2 datos en lugar de s√≥lo uno.
Tambi√©n se han a√±adido t√©cnicas de Mixed Precision (Narang et al.~2018),
los Tensor Cores (para las gr√°ficas de NVIDIA) son otra de las mejoras
que se han ido incorporando a la GPUs y que permiten acelerar los
procesos tanto de entrenamiento como de predicci√≥n con las redes
neuronales.

El primer objetivo ser√° convertir las variables categ√≥ricas en variables
num√©ricas, de forma que el AE pueda trabajar con ellas. Para realizar la
conversi√≥n de categ√≥rica a num√©rica b√°sicamente tenemos dos m√©todos para
realizarlo: ‚Ä¢ Codificaci√≥n one-hot. ‚Ä¢ Codificaci√≥n entera. La
codificaci√≥n one-hot consiste en crear tantas variables como categor√≠as
tenga la variable, de forma que se asigna el valor 1 si tiene esa
categor√≠a y el 0 si no la tiene.

La codificaci√≥n entera lo que hace es codificar con un n√∫mero cada
categor√≠a. Realmente esta asignaci√≥n no tiene ninguna interpretaci√≥n
num√©rica ya que en general las categor√≠as no tienen porque representar
un orden al que asociarlas. Normalmente se trabaja con codificaci√≥n
one-hot para representar los datos categ√≥ricos de forma que ser√°
necesario preprocesar los datos de partida para realizar esta
conversi√≥n, creando tantas variables como categor√≠as haya por cada
variable. Si nosotros tenemos nuestra muestra de datos de que tiene
variables de forma que , y son variables categ√≥ricas que tienen n√∫mero
de categor√≠as respectivamente, tendremos finalmente las siguientes
variables s√≥lo num√©ricas:

De esta forma, se aumentar√°n el n√∫mero de variables con las que vamos a
trabajar en funci√≥n de las categor√≠as que tengan las variables
categ√≥ricas. Normalmente nos encontramos que en una red neuronal las
variables de salida son: ‚Ä¢ un n√∫mero (regresi√≥n) ‚Ä¢ una serie de n√∫meros
(regresi√≥n m√∫ltiple) ‚Ä¢ un dato binario (clasificaci√≥n binaria) ‚Ä¢ una
serie de datos binarios que representa una categor√≠a de varias
(clasifiaci√≥n m√∫ltiple)

\textbf{Arquitectura de red}

Para la construcci√≥n de una red neuronal necesitamos definir la
arquitectura de esa red. Esta arquitectura, si estamos pensando en una
red neuronal densamente conectada, estar√° definida por la cantidad de
capas ocultas y el n√∫mero de neuronas que tenemos en cada capa. M√°s
adelante veremos que dependiendo del tipo de red neuronal podr√° haber
otro tipo de elementos en estas capas. Funci√≥n de coste y p√©rdida Otro
de los elementos clave que tenemos que tener en cuenta a la hora de usar
nuestra red neuronal son las funciones de p√©rdida y funciones de coste
(objetivo). La funci√≥n de p√©rdida va a ser la fuci√≥n que nos dice c√≥mo
de diferente es el resultado del dato que nosotros quer√≠amos conseguir
respecto al dato original. Normalmente se suelen usar diferentes tipos
de funciones de p√©rdida en funci√≥n del tipo de resultado con el que se
vaya a trabajar. La funci√≥n de coste es la funci√≥n que vamos a tener que
optimizar para conseguir el m√≠nimo valor posible, y que recoge el valor
de la funci√≥n de p√©rdida para toda la muestra. Tanto las funciones de
p√©rdida como las funciones de coste, son funciones que devuelven valores
de .

Si tenemos un problema de regresi√≥n en el que tenemos que predecir un
valor o varios valores num√©ricos, algunas de las funciones a usar son: ‚Ä¢
Error medio cuadr√°tico () {[}120{]} , es el valor real e es el valor
predicho ‚Ä¢ Error medio absoluto () {[}121{]} , es el valor real e es el
valor predicho Para los problemas de clasifiaci√≥n: ‚Ä¢ Binary Crossentropy
(S√≥lo hay dos clases) {[}122{]} es el valor real e es el valor predicho
‚Ä¢ Categorical Crosentropy (M√∫ltiples clases representadas como one-hot)
{[}123{]} es el valor real para la clase e es el valor predicho para la
clase ‚Ä¢ Sparse Categorical Crossentropy (M√∫ltiples clases representadas
como un entero) {[}124{]} es el valor real para la clase e es el valor
predicho para la clase ‚Ä¢ Kullback-Leibler Divergence Esta funci√≥n se usa
para calcular la diferencia entre dos distribuciones de probabilidad y
se usa por ejemplo en algunas redes como Variational Autoencoders
(Doersch 2016) o Modelos GAN (Generative Adversarial Networks) {[}125{]}
{[}126{]} {[}127{]} ‚Ä¢ Hinge Loss {[}128{]} Las correspondientes
funciones de coste que se usar√≠an, estar√≠an asociadas a todas las
muestras que se est√©n entrenando o sus correpondientes batch, as√≠ como
posibles t√©rminos asociados a la regularizaci√≥n para evitar el
sobreajuste del entrenamiento. Es decir, la funci√≥n de p√©rdida se
calcula para cada muestra, y la funci√≥n de coste es la media de todas
las muestras. Por ejemplo, para el Error medio cuadr√°tico () tendr√≠amos
el siguiente valor: {[}129{]}

\textbf{Optimizador}

El Descenso del gradiente es la versi√≥n m√°s b√°sica de los algoritmos que
permiten el aprendizaje en la red neuronal haciendo el proceso de
backpropagation (propagaci√≥n hacia atr√°s). A continuaci√≥n veremos una
breve explicaci√≥n del algoritmo as√≠ como algunas variantes del mismo
recogidas en (Ruder 2017) Recordamos que el descenso del gradiente nos
permitir√° actualizar los par√°metros de la red neuronal cada vez que
demos una pasada hacia delante con todos los datos de entrada, volviendo
con una pasada hacia atr√°s. {[}130{]} donde es la funci√≥n de coste, es
el par√°metro de ratio de aprendizaje que permite definir como de grandes
se quiere que sean los pasos en el aprendizaje. Cuando lo que hacemos es
actualizar los par√°metros para cada pasada hacia delante de una sola
muestra, estaremos ante lo que llamamos Stochastic Gradient Descent
(SGD). En este proceso converger√° en menos iteraciones, aunque puede
tener alta varianza en los par√°metros. {[}131{]} donde e son los valores
en la pasada de la muestra . Podemos buscar un punto intermedio que
ser√≠a cuando trabajamos por lotes y cogemos un bloque de datos de la
muestra, les aplicamos la pasada hacia delante y aprendemos los
par√°metros para ese bloque. En este caso lo llamaremos Mini-batch
Gradient Descent {[}132{]} donde son los valores de ese batch . En
general a estos m√©todos nos referiremos a ellos como SGD. Sobre este
algoritmo base se han hecho ciertas mejoras como: \textbf{Learning rate
decay} Podemos definir un valor de decenso del ratio de aprendizaje, de
forma que normalmente al inicio de las iteraciones de la red neuronal
los pasos ser√°n m√°s grandes, pero conforme nos acercamos a la soluci√≥n
optima deberemos dar pasos m√°s peque√±os para ajustarnos mejor. {[}133{]}
donde ahora se ir√° reduciendo en funci√≥n del valor del decay Momentum El
\textbf{momentum} se introdujo para suavizar la convergencia y reducir
la alta varianza de SGD. {[}134{]} {[}135{]} donde es lo que se llama el
vector velocidad con la direcci√≥n correcta. \textbf{NAG (Nesterov
Accelerated Gradient)} Ahora daremos un paso m√°s con el NAG, calculando
la funci√≥n de coste junto con el vector velocidad. {[}136{]} {[}137{]}
donde ahora vemos que la funci√≥n de coste se calcula usando los
par√°metros de sumado a

Veamos algunos algoritmos de optimizaci√≥n m√°s que, aunque provienen del
SGD, se consideran independientes a la hora de usarlos y no como
par√°metros extras del SGD. \textbf{Adagrad (Adaptive Gradient)} Esta
variante del algoritmo lo que hace es adaptar el ratio de aprendizaje
para cada uno de los pesos en lugar de que sea global para todos.
{[}138{]} donde tenemos que es una matriz diagonal donde cada elemento
es la suma de los cuadrados de los gradientes en el paso , y es un
t√©rmino de suavizado par evitar divisiones por 0.

\textbf{RMSEProp (Root Mean Square Propagation)} En este caso tenemos
una variaci√≥n del Adagrad en el que intenta reducir su agresividad
reduciendo monotonamente el ratio de aprendizaje. En lugar de usar el
gradiente acumulado desde el principio de la ejecuci√≥n, se restringe a
una ventana de tama√±o fijo para los √∫ltimos n gradientes calculando su
media. As√≠ calcularemos primero la media en ejecuci√≥n de los cuadros de
los gradientes como: {[}139{]} y luego ya pasaremos a usar este valor en
la actualizaci√≥n {[}140{]}

\textbf{AdaDelta}

Aunque se desarrollaron de forma simult√°nea el AdaDelta y el RMSProp son
muy parecidos en su primer paso incial, llegando el de AdaDelta un poco
m√°s lejos en su desarrollo. {[}141{]} y luego ya pasaremos a usar este
valor en la actualizaci√≥n {[}142{]} {[}143{]} \textbf{Adam (Adaptive
Moment Estimation)} {[}144{]} {[}145{]} {[}146{]} donde y son
estimaciones del primer y segundo momento de los gradientes
respectivamente, y y par√°metros a asignar. {[}147{]} {[}148{]} {[}149{]}
\textbf{Adamax} {[}150{]} {[}151{]} {[}152{]} {[}153{]} donde y son
estimaciones del primer y segundo momento de los gradientes
respectivamente, y y par√°metros a asignar. {[}154{]} {[}155{]}
\textbf{Nadam (Nesterov-accelerated Adaptive Moment Estimatio)} Combina
Adam y NAG. {[}156{]} {[}157{]} {[}158{]}

\textbf{Funci√≥n de activaci√≥n} Las funciones de activaci√≥n dentro de una
red neuronal son uno de los elementos clave en el dise√±o de la misma.
Cada tipo de funci√≥n de activaci√≥n podr√° ayudar a la convergencia de
forma m√°s o menos r√°pida en funci√≥n del tipo de problema que se plantee.
En un AE las funciones de activaci√≥n en las capas ocultas van a
conseguir establecer las restricciones no lineales al pasar de una capa
a la siguiente, normalmente se evita usar la funci√≥n de activaci√≥n
lineal en las capas intermedias ya que queremos conseguir
transformaciones no lineales. A continuaci√≥n, exponemos las principales
funciones de activaci√≥n que mejores resultados dan en las capas ocultas:
‚Ä¢ Paso binario (Usado por los primeros modelos de neuronas) {[}159{]} ‚Ä¢
Identidad {[}160{]} ‚Ä¢ Sigmoid (Log√≠stica) {[}161{]} ‚Ä¢ Tangente
Hiperb√≥lica (Tanh) {[}162{]} ‚Ä¢ Softmax {[}163{]}

\begin{verbatim}
‚Ä¢ ReLu ( Rectified Linear Unit)
    [164]

‚Ä¢ LReLU (Leaky Rectified Linear Unit)
    [165]
‚Ä¢ PReLU (Parametric Rectified Linear Unit)
    [166]
‚Ä¢ RReLU (Randomized Rectified Linear Unit)
    [167]
\end{verbatim}

*La diferencia entre LReLu, PReLu y RRLeLu es que en LReLu el par√°metro
es uno que se asigna fijo, en el caso de PReLu el par√°metro tambi√©n se
aprende durante el entrenamiento y finalmente en RReLu es un par√°metro
con valores entre 0 y 1, que se obtiene de un muestreo en una
distribuci√≥n normal. Se puede profundizar en este grupo de funciones de
activaci√≥n en (Xu et al.~2015) ‚Ä¢ ELU (Exponential Linear Unit) {[}168{]}
FIGURA n¬∫ 64: COMPARACI√ìN ENTRE LAS FUNCIONES ReLU, LReLU/PReLU, RReLU y
ELU

FUENTE: Jiuxiang, G. et al (2019)

\textbf{Funci√≥n de activaci√≥n en salida} En la capa de salida tenemos
que tener en cuenta cual es el tipo de datos final que queremos obtener,
y en funci√≥n de eso elegiremos cual es la funci√≥n de activaci√≥n de
salida que usaremos. Normalmente las funciones de activaci√≥n que se
usar√°n en la √∫ltima capa seran: ‚Ä¢ Lineal con una unidad, para regresi√≥n
de un solo dato num√©rico {[}169{]} donde es un valor escalar. ‚Ä¢ Lineal
con multiples unidades, para regresi√≥n de varios datos num√©ricos
{[}170{]} donde es un vector. ‚Ä¢ Sigmoid para clasifiaci√≥n binaria
{[}171{]} ‚Ä¢ Softmax para calsifiaci√≥n m√∫ltiple {[}172{]}

\textbf{Regularizaci√≥n} Las t√©cnicas de regularizaci√≥n nos permiten
conseguir mejorar los problemas que tengamos por sobreajuste en el
entrenamiento de nuestra red neuronal. A continuaci√≥n, vemos algunas de
las t√©cnicas de regularizaci√≥n existentes en la actualidad: ‚Ä¢ Norma LP
B√°sicamente estos m√©todos tratan de hacer que los pesos de las neuronas
tengan valores muy peque√±os consiguiendo una distribuci√≥n de pesos m√°s
regular. Esto lo consiguen al a√±adir a la funci√≥n de p√©rdida un coste
asociado a tener pesos grandes en las neuronas. Este peso se puede
construir o bien con la norma L1 (proporcional al valor absoluto) o con
la norma L2 (proporcional al cuadrado de los coeficientes de los pesos).
En general se define la norma LP) {[}173{]} {[}174{]} Para los casos m√°s
habituales tendr√≠amos la norma L1 y L2. {[}175{]} {[}176{]}

\textbf{Dropout} Una de las t√©cnicas de regularizaci√≥n que m√°s se est√°n
usando actualmente es la llamada Dropout, su proceso es muy sencillo y
consiste en que en cada iteraci√≥n de forma aleatoria se dejan de usar un
porcentaje de las neuronas de esa capa, de esta forma es m√°s dificil
conseguir un sobreajuste porque las neuronas no son capaces de memorizar
parte de los datos de entrada. \textbf{Dropconnect} El Dropconnect es
otra t√©cnica que va un poco m√°s all√° del concepto de Dropout y en lugar
de usar en cada capa de forma aleatoria una serie de neuronas, lo que se
hace es que de forma aleatoria se ponen los pesos de la capa a cero. Es
decir, lo que hacemos es que hay ciertos enlaces de alguna neurona de
entrada con alguna de salida que no se activan.

\textbf{Inicializaci√≥n de pesos}

Cuando empieza el entrenamiento de una red neuronal y tiene que realizar
la primera pasada hacia delante de los datos, necesitamos que la red
neuronal ya tenga asignados alg√∫n valor a los pesos. Se pueden hacer
inicializaciones del tipo: ‚Ä¢ Ceros Todos los pesos se inicializan a 0. ‚Ä¢
Unos Todos los pesos se inicializan a 1. ‚Ä¢ Distribuci√≥n normal Los pesos
se inicializan con una distribuci√≥n normal, normalmente con media 0 y
una desviaci√≥n alrededor de 0,05. Es decir, valores bastante cercanos al
cero. ‚Ä¢ Distribuci√≥n normal truncada Los pesos se inicializan con una
distribuci√≥n normal, normalmente con media 0 y una desviaci√≥n alrededor
de 0,05 y adem√°s se truncan con un m√°ximo del doble de la desviaci√≥n.
Los valores aun s√≥n m√°s cercanos a cero. ‚Ä¢ Distribuci√≥n uniforme Los
pesos se inicializan con una distribuci√≥n uniforme, normalmente entre el
0 y el 1. ‚Ä¢ Glorot Normal (Tambi√©n llamada Xavier normal) Los pesos se
inicializan partiendo de una distribuci√≥n normal truncada en la que la
desivaci√≥n es donde es el n√∫mero de unidades de entrada y fanout es el
n√∫mero de unidades de salida. Ver (Glorot and Bengio 2010) ‚Ä¢ Glorot
Uniforme (Tambi√©n llamada Xavier uniforme) Los pesos se inicializan
partiendo de una distribuci√≥n uniforme done los l√≠mites son done y es el
n√∫mero de unidades de entrada y fanout es el n√∫mero de unidades de
salida. Ver (Glorot and Bengio 2010)

\textbf{Batch normalization} Hemos comentado que cuando entrenamos una
red neuronal los datos de entrada deben ser todos de tipo num√©rico y
adem√°s los normalizamos para tener valores ``cercanos a cero'', teniendo
una media de 0 y varianza de 1, consiguiendo uniformizar todas las
variables y conseguir que la red pueda converger m√°s f√°cilmente. Cuando
los datos entran a la red neuronal y se comienza a operar con ellos, se
convierten en nuevos valores que han perdido esa propiedad de
normalizaci√≥n. Lo que hacemos con la normalizaci√≥n por lotes (batch
normalization) (Ioffe and Szegedy 2015) es que a√±adimos un paso extra
para normalizar las salidas de las funciones de activaci√≥n. Lo normal es
que se aplicara la normalizaci√≥n con la media y la varianza de todo el
bloque de entrenamiento en ese paso, pero normalmente estaremos
trabajando por lotes y se calcular√° la media y varianza con ese lote de
datos.

\hypertarget{principales-arquitecturas-de-deep-learning}{%
\subsection{Principales arquitecturas de Deep
Learning}\label{principales-arquitecturas-de-deep-learning}}

Actualmente existen muchos tipos de estructuras de redes neuronales
artificiales dado que logran resultados extraordinarios en muchos campos
del conocimiento. Los primeros √©xitos en el aprendizaje profundo se
lograron a trav√©s de las investigaciones y trabajos de Geoffre Hinton
(2006) que introduce las Redes de Creencia Profunda en cada capa de la
red de una M√°quina de Boltzmann Restringida (RBM) para la asignaci√≥n
inicial de los pesos sin√°pticos. Hace tiempo que se est√° trabajando con
arquitecturas como los Autoencoders, Hinton y Zemel (1994), las RBMs de
Hinton y Sejnowski (1986) y las DBNs (Deep Belief Networks), Hinton et
al.~(2006) y otras como las redes recurrentes y convolucionales. Estas
t√©cnicas constituyen en s√≠ mismas arquitecturas de redes neuronales,
aunque tambi√©n algunas de ellas, como se ha afirmado en la introducci√≥n,
se est√°n empleando para inicializar los pesos de arquitecturas profundas
de redes neuronales supervisadas con conexiones hacia adelante. Las
principales arquitecturas de deep learning se resumen en la siguiente
figura. Figura 65. modelos de redes neuronales seg√∫n tipo de aprendizaje

Vamos a describir de forma somera las principales arquitecturas dado que
posteriormente se desarrollan m√°s ampliamente en este ep√≠grafe o se
tratan en docuemento adjunto que se acompa√±a en este m√≥dulo
\textbf{Convolutional Neural Network} Tal vez los modelos m√°s utilizados
actualmente en el campo del Deep Learning sean las redes neuronales
convolucionales, denominadas en ingl√©s Convolutional Neural Networks
(CNN). El objetivo de las redes CNN es aprender caracter√≠sticas de orden
superior utilizando la operaci√≥n de convoluci√≥n. Estas estructuras de
redes neuronales son especialmente eficaces para clasificar y segmentar
im√°genes, en general, son notablemente eficaces en tareas de visi√≥n
artificial. Las CNN son una modificaci√≥n del perceptr√≥n multicapa
explicado en un m√≥dulo anterior. Este modelo es muy similar al trabajo
que ejecuta el cerebro humano: las neuronas se corresponden a campos
receptivos similares a como lo realizan las neuronas en la corteza
visual de nuestro cerebro. Esta arquitectura ya se utiliz√≥ en 1990 donde
la empresa AT \& T las aplic√≥ para crear un modelo de lectura de
cheques, desarroll√°ndose posteriormente muchos sistemas OCR basados en
CNN. Las redes de convoluci√≥n tienen una estructura de varias capas: las
capas de Convoluci√≥n que transforman los datos de entrada a trav√©s de
una operaci√≥n matem√°tica llamada Convoluci√≥n y la capa de pooling, que
trata de sintetizar y condensar la informaci√≥n de la capa de
convoluci√≥n. Finalmente, se transforman los datos para aplicar una red
densamente conectada que nos ofrece el resultado final en relaci√≥n con
el objetivo que se busca. Estas redes neuronales artificiales se
desarrollaron al abrigo de los concursos denominados ILSVRC (Large Scale
Visual Recognition Challenge) donde aparecieron las principales
aportaciones efectuadas en las redes convolucionales y que hoy podemos
utilizar todos los investigadores. Algunos de las estructuras m√°s
novedosas y que son modelos ya preentrenados, con estructuras de capas
m√°s numerosas y que podemos integrar en nuestras aplicaciones, se
denominan: LeNet-5, AlexNet, VGG, GoogLeNet y Resnet.

\textbf{Autoencoder} Los Autoencoders (AE) son uno de los tipos de redes
neuronales que caen dentro del √°mbito del Deep Learning, en la que nos
encontramos con un modelo de aprendizaje no supervisado. Ya se empez√≥ a
hablar de AE en la d√©cada de los 80 (Bourlard and Kamp 1988), aunque es
en estos √∫ltimos a√±os donde m√°s se est√° trabajando con ellos. La
arquitectura de un AE es una Red Neuronal Artificial (ANN por sus siglas
en ingl√©s) que se encuentra dividida en dos partes, encoder y decoder
(Charte et al.~2018), (Goodfellow, Bengio, and Courville 2016). El
encoder va a ser la parte de la ANN que va codificar o comprimir los
datos de entrada, y el decoder ser√° el encargado de regenerar de nuevo
los datos en la salida. Esta estructura de codificaci√≥n y decodificaci√≥n
le llevar√° a tener una estructura sim√©trica. El AE es entrenado para ser
capaz de reconstruir los datos de entrada en la capa de salida de la
ANN, implementando una serie de restricciones (la reducci√≥n de elementos
en las capas ocultas del encoder) que van a evitar que simplemente se
copie la entrada en la salida. Algunas de sus principales aplicaciones
sobre las que se est√° investigando son: ‚Ä¢ Reducci√≥n de dimensiones /
Compresi√≥n de datos ‚Ä¢ B√∫squeda de im√°genes ‚Ä¢ Detecci√≥n de Anomal√≠as ‚Ä¢
Eliminaci√≥n de ruido \textbf{Redes recurrentes} En la actualidad las
\textbf{redes neuronales recurrentes (Recurrent Neural Networks)} han
logrado un puesto destacado en machine learning. Estas redes que no
disponen de una estructura de capas, sino que permiten conexiones
arbitrarias entre todas las neuronas, incluso creando ciclos. En esta
arquitectura se permiten conexiones recurrentes lo que aumenta el n√∫mero
de pesos o de par√°metros ajustables de la red, lo que incrementa la
capacidad de representaci√≥n, pero tambi√©n la complejidad del
aprendizaje. Las peculiaridades de esta red permiten incorporar a la red
el concepto de temporalidad, y tambi√©n que la red tenga memoria, porque
los n√∫meros que introducimos en un momento dado en las neuronas de
entrada son transformados, y contin√∫an circulando por la red. Existen
diferentes planteamientos de redes recurrentes, por ejemplo, son muy
populares por sus aplicaciones las redes de Elmann y de Jordan. En los
√∫ltimos a√±os se han popularizado las redes recurrentes denominadas
Long-Short Term Memory (LSTM) que son una extension de las redes
recurrentes y su caracter√≠tica principal es que amplian su memoria para
registrar experiencias que han ocurrido hace mucho tiempo. Normalmente
contienen tres puertas que determinan si se permite o no una nueva
entrada o se elimina la informaci√≥n que llega dado que se considera no
importante. Son an√°logas a una funci√≥n sigmoide lo que implica que van
de 0 a 1, lo que permite incorporarlas al proceso de backpropagation.
Tambi√©n se encuentran entre las redes recurrentes, las denominadas GRU
(Gated Recurrent Unit) que aparecieron en 2014 y simplifican a las LSMT:
son computacionalmente menos costosas y m√°s eficientes.

Boltzmann Machine y Restricted Boltzmann Machine El aprendizaje de la
denominada m√°quina de Boltzmann (BM) se realiza a trav√©s de un algoritmo
estoc√°stico que proviene de ideas basadas en la mec√°nica estad√≠stica.
Este prototipo de red neuronal tiene una caracter√≠stica distintiva y es
que el uso de conexiones sin√°pticas entre las neuronas es sim√©trico. Las
neuronas son de dos tipos: visibles y ocultas. Las neuronas visibles son
las que interact√∫an y proveen una interface entre la red y el ambiente
en el que operan, mientras que las neuronas act√∫an libremente sin
interacciones con el entorno. Esta m√°quina dispone de dos modos de
operaci√≥n. El primero es la condici√≥n de anclaje donde las neuronas
est√°n fijas por los est√≠mulos espec√≠ficos que impone el ambiente. El
otro modo es la condici√≥n de libertad, donde tanto las neuronas ocultas
como las visibles act√∫an libremente sin condiciones impuestas por el
medio ambiente. Las maquinas restringidas de Boltzmann (RBM) solamente
toman en cuenta aquellos modelos en los que no existen conexiones del
tipo visible-visible y oculta-oculta. Estas redes tambi√©n asumen que los
datos de entrenamiento son independientes y est√°n id√©nticamente
distribuidos. Una forma de estimar los par√°metros de un modelo
estoc√°stico es calculando la m√°xima verosimilitud. Para ello, se hace
uso de los Markov Random Fiels (MRF), ya que al encontrar los par√°metros
que maximizan los datos de entrenamiento bajo una distribuci√≥n MRF,
equivale a encontrar los par√°metros ùúÉ que maximizan la verosimilitud de
los datos de entrenamiento, Fischer e Igel (2012). Maximizar dicha
verosimilitud es el objetivo que persigue el algoritmo de entrenamiento
de una RBM. A pesar de utilizar la distribuci√≥n MRF, computacionalmente
hablando se llega a ecuaciones inviables de implementar. Para evitar el
problema anterior, las esperanzas que se obtienen de MRF pueden ser
aproximadas por muestras extra√≠das de distribuciones basadas en las
t√©cnicas de Markov Chain Monte Carlo Techniques (MCMC). Las t√©cnicas de
MCMC utilizan un algoritmo denominado muestreo de Gibbs con el que
obtenemos una secuencia de observaciones o muestras que se aproximan a
partir de una distribuci√≥n de verosimilitud de m√∫ltiples variables
aleatorias. La idea b√°sica del muestreo de Gibss es actualizar cada
variable posteriormente en base a su distribuci√≥n condicional dado el
estado de las otras variables. Deep Belief Network Una red Deep Belief
Network tal como demostr√≥ Hinton se puede considerar como un
``apilamiento de redes restringidas de Boltzmann''. Tiene una estructura
jer√°rquica que como sabemos es una de las caracter√≠sticas del deep
learning. Como en el anterior modelo, esta red tambi√©n es un modelo en
grafo estoc√°stico, que aprende a extraer una representaci√≥n jer√°rquica
profunda de los datos de entrenamiento. Cada capa de la RBM extrae un
nivel de abstracci√≥n de caracter√≠sticas de los datos de entrenamiento,
cada vez m√°s significativo; pero para ello, la capa siguiente necesita
la informaci√≥n de la capa anterior lo que implica el uso de las
variables latentes. Estos modelos caracterizan la distribuci√≥n conjunta
hk entre el vector de observaciones x y las capas ocultas, donde x=h0,
es una distribuci√≥n condicional para las unidades visibles limitadas
sobre las unidades ocultas que pertenecen a la RBM en el nivel k, y es
la distribuci√≥n conjunta oculta visible en la red RBM del nivel superior
o de salida. El entrenamiento de esta red puede ser h√≠brido, empezando
por un entrenamiento no supervisado para despu√©s aplicar un
entrenamiento supervisado para un mejor y m√°s √≥ptimo ajuste, aunque
pueden aplicarse diferentes tipos de entrenamiento, Bengio et al.~(2007)
y Salakhutdinov (2014) Para realizar un entrenamiento no supervisado se
aplica a las redes de creencia profunda con Redes restringidas de
Boltzmann el m√©todo de bloque constructor que fue presentado por Hinton
(2006) y por Bengio (2007)

\hypertarget{redes-neuronales-convolucionales}{%
\section{Redes Neuronales
Convolucionales}\label{redes-neuronales-convolucionales}}

\hypertarget{introducciuxf3n-2}{%
\subsection{Introducci√≥n}\label{introducciuxf3n-2}}

Esta arquitectura de redes de neuronas convolucionales, CNN,
Convolutional Neural Networks es en la actualidad el campo de
investigaci√≥n m√°s fecundo dentro de las redes neuronales artificiales de
Deep learning y donde los investigadores, empresas e instituciones est√°n
dedicando m√°s recursos e investigaci√≥n. Para apoyar esta aseveraci√≥n, en
google trend se observa que el t√©rmino convolutional neural network en
relaci√≥n con el concepto de artificial neural network crece y est√° por
encima desde el a√±o 2016. Es en este √∫ltimo lustro donde el Deep
learning ha tomado una importancia considerable.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/busqueda_google.png}

}

\caption{\label{fig-busqueda-google}b√∫squeda de t√©rminos de redes
neuronales en google trend}

\end{figure}

Fuente: Google Trend En este modelo de redes convolucionales las
neuronas se corresponden a campos receptivos similares a las neuronas en
la corteza visual de un cerebro humano. Este tipo de redes se han
mostrado muy efectivas para tareas de detecci√≥n y categorizaci√≥n de
objetos y en la clasificaci√≥n y segmentaci√≥n de im√°genes. Por ejemplo,
estas redes en la d√©cada de 1990 las aplic√≥ AT \& T para desarrollar un
modelo para la lectura de cheques. Tambi√©n m√°s tarde se desarrollaron
muchos sistemas OCR basados en CNN. En esta arquitectura cada neurona de
una capa no recibe conexiones entrantes de todas las neuronas de la capa
anterior, sino s√≥lo de algunas. Esta estrategia favorece que una neurona
se especialice en una regi√≥n del conjunto de n√∫meros (p√≠xeles) de la
capa anterior, lo que disminuye notablemente el n√∫mero de pesos y de
operaciones a realizar. Lo m√°s normal es que neuronas consecutivas de
una capa intermedia se especialicen en regiones solapadas de la capa
anterior. Una forma intuitiva para entender c√≥mo trabajan estas redes
neuronales es ver c√≥mo nos representamos y vemos las im√°genes. Para
reconocer una cara primero tenemos que tener una imagen interna de lo
que es una cara. Y a una imagen de una cara la reconocemos porque tiene
nariz, boca, orejas, ojos, etc. Pero en muchas ocasiones una oreja est√°
tapada por el pelo, es decir, los elementos de una cara se pueden
ocultar de alguna manera. Antes de clasificarla, tenemos que saber la
proporci√≥n y disposici√≥n y tambi√©n c√≥mo se relacionan la partes entre
s√≠. Para saber si las partes de la cara se encuentran en una imagen
tenemos que identificar previamente l√≠neas bordes, formas, texturas,
relaci√≥n de tama√±o, etc√©tera. En una red convolucional, cada capa lo que
va a ir aprendiendo son los diferentes niveles de abstracci√≥n de la
imagen inicial. Para comprender mejor el concepto anterior hemos
seleccionado esta imagen de Raschka y Mirjalili (2019) donde se observa
como partes del perro se transforman en neuronas del mapa de
caracter√≠sticas

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/correspondencia_featrures.png}

}

\caption{\label{fig-correspondencia-features}Correspondencia de zonas de
la imagen y mapa de caracter√≠sticas}

\end{figure}

Fuente: Raschka y Mirjalili (2019)

El objetivo de las redes CNN es aprender caracter√≠sticas de orden
superior utilizando la operaci√≥n de convoluci√≥n. Puesto que las redes
neuronales convolucionales pueden aprender relaciones de entrada-salida
(donde la entrada es una imagen), en la convoluci√≥n, cada pixel de
salida es una combinaci√≥n lineal de los pixeles de entrada. La
convoluci√≥n consiste en filtrar una imagen utilizando una m√°scara.
Diferentes m√°scaras producen distintos resultados. Las m√°scaras
representan las conexiones entre neuronas de capas anteriores. Estas
capas aprenden progresivamente las caracter√≠sticas de orden superior de
la entrada sin procesar. Las redes neuronales convolucionales se forman
usando dos tipos de capas: convolucionales y pooling. La capa de
convoluci√≥n transforma los datos de entrada a trav√©s de una operaci√≥n
matem√°tica llamada convoluci√≥n. Esta operaci√≥n describe c√≥mo fusionar
dos conjuntos de informaci√≥n diferentes. A esta operaci√≥n se le suele
aplicar una funci√≥n de transformaci√≥n, generalmente la RELU. Despu√©s de
la capa o capas de convoluci√≥n se usa una capa de pooling, cuya funci√≥n
es resumir las respuestas de las salidas cercanas. Antes de obtener el
output unimos la √∫ltima capa de pooling con una red densamente
conectada. Previamente se ha aplanado (Flatering) la √∫ltima capa de
pooling para obtener un vector de entrada a la red neural final que nos
ofrecer√° los resultados.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/arquitectura_convolucional.png}

}

\caption{\label{fig-arquitectura_convolucional}Arquitectura de una CNN}

\end{figure}

Las redes neuronales convolucionales debido a su forma de concebirse son
aptas para poder aprender a clasificar todo tipo de datos donde √©stos
est√©n distribuidos de una forma continua a lo largo del mapa de entrada,
y a su vez sean estad√≠sticamente similares en cualquier lugar del mapa
de entrada. Por esta raz√≥n, son especialmente eficaces para clasificar
im√°genes. Tambi√©n pueden ser aplicadas para la clasificaci√≥n de series
de tiempo o se√±ales de audio. En relaci√≥n con el color y la forma de
codificarse, en las redes convolucionales se realiza en tensores 3D, dos
ejes para el ancho (width) y el alto (height) y el otro eje llamado de
profundidad (depht) que es el canal del color con valor tres si
trabajamos con im√°genes de color RGB (Red, Green y Blue) rojo, verde y
azul. Si disponemos de im√°genes en escala de grises el valor de depht es
uno. La base de datos MNIST (National Institute of Standards and
Technology database) con la que trabajaremos en este ep√≠grafe contiene
im√°genes de 28 x 28 pixeles, los valores de height y de widht son ambos
28, y al ser una base de datos en blanco y negro el valor de depht es 1.
Las im√°genes son matrices de p√≠xeles que van de cero a 255 y que para la
red neuronal se normalizan para que sus valores oscilen entre cero y
uno. 7.4.2. Convoluci√≥n En las redes convolucionales todas las neuronas
de la capa de entrada (los p√≠xeles de las im√°genes) no se conectan con
todas las neuronas de la capa oculta del primer nivel como lo hacen las
redes cl√°sicas del tipo perceptr√≥n multicapa o las redes que conocemos
de forma gen√©rica como redes densamente conectadas. Las conexiones se
realizan por peque√±as zonas de la capa de entrada.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/conexion_neuronas_convolucional.png}

}

\caption{\label{fig-conexion-neuronas-convolucional}Conexi√≥n de las
neuronas de la capa de entrada con la capa oculta}

\end{figure}

Veamos un ejemplo para la base de datos de los d√≠gitos del 1 a 9. Vamos
a conectar cada neurona de la capa oculta con una regi√≥n de 5 x 5
neurona, es decir, con 25 neuronas de la capa de entrada, que podemos
denominarla ventana. Esta ventana va a ir recorriendo todo el espacio de
entrada de 28 x 28 empezando por arriba y desplaz√°ndose de izquierda a
derecha y de arriba abajo. Suponemos que los desplazamientos de la
ventana son de un paso (un pixel) aunque este es un par√°metro de la red
que podemos modificar (en la programaci√≥n lo llamaremos stride). Para
conectar la capa de entrada con la de salida utilizaremos una matriz de
pesos (W) de tama√±o 3 x 3 que recibe el nombre de filtro (filter) y el
valor del sesgo. Para obtener el valor de cada neurona de la capa oculta
realizaremos el producto escalar entre el filtro y la ventana de la capa
de entrada. Utilizamos el mismo filtro para obtener todas las neuronas
de la capa oculta, es decir en todos los productos escalares siempre
utilizamos la misma matriz, el mismo filtro. Se definen matem√°ticamente
estos productos escalares a trav√©s de la siguiente expresi√≥n: {[}177{]}

Como en este tipo de red un filtro s√≥lo nos permite revelar una
caracter√≠stica muy concreta de la imagen, lo que se propone es usar
varios filtros simult√°neamente, uno para cada caracter√≠stica que
queramos detectar. Una forma visual de representarlo (si suponemos que
queremos aplicar 32 filtros) es como se muestra a continuaci√≥n:

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/primera_capa_convolucional.png}

}

\caption{\label{fig-primera-capa-convolucional}Primera capa de la red
convolucional con 32 filtros}

\end{figure}

Al resultado de la aplicaci√≥n de los diferentes filtros se les suele
aplicar la funci√≥n de activaci√≥n denominada RELU y que ya se coment√≥ en
la introducci√≥n. Una interesante fuente de informaci√≥n es la
documentaci√≥n del software gratuito GIMP donde expone diferentes efectos
que se producen en las im√°genes al aplicar diversas convoluciones. Un
ejmplo claro y did√°ctico lo podemos obtener de la docuemntaci√≥n del
software libre de dibujo y tratamiento de im√°genes denominado GIMP
(https://docs.gimp.org/2.6/es/plug-in-convmatrix.html). Algunos de estos
efectos nos ayudan a entender la operaci√≥n de los filtros en las redes
convolucionales y c√≥mo afectan a las im√°genes, en concreto, el ejemplo
que presenta lo realiza sobre la figura del Taj Mahal El filtro enfocar
lo que consigue es afinar los rasgos, los contornos lo que nos permite
agudizar los objetos de la imagen. Toma el valor central de la matriz de
cinco por cinco lo multiplica por cinco y le resta el valor de los
cuatro vecinos. Al final hace una media, lo que mejora la resoluci√≥n del
pixel central porque elimina el ruido o perturbaciones que tiene de sus
pixeles vecinos. El filtro enfocar (Sharpen)

Lo contario al filtro enfocar lo obtenemos a trav√©s de la matriz
siguiente, difuminando la imagen al ser estos p√≠xeles mezclados o
combinados con los pixeles cercanos. Promedia todos los p√≠xeles vecinos
a un pixel dado lo que implica que se obtienen bordes borrosos. Filtro
desenfocar

Filtro Detectar bordes (Edge Detect) Este efecto se consigue mejorando
los l√≠mites o las aristas de la imagen. En cada p√≠xel se elimina su
vecino inmediatamente anterior en horizontal y en vertical. Se eliminan
las similitudes vecinas y quedan los bordes resaltados. Al pixel central
se le suman los cuatro p√≠xeles vecinos y lo que queda al final es una
medida de c√≥mo de diferente es un p√≠xel frente a sus vecinos. En el
ejemplo, al hacer esto da un valor de cero de ah√≠ que se observen tantas
zonas oscuras.

Filtro Repujado (Emboss) En este filto se observa que la matriz es
sim√©trica y lo que intenta a trav√©s del dise√±o del filtro es mejorar los
p√≠xeles centrales y de derecha abajo rest√°ndole los anteriores. Se
obtiene lo que en fotograf√≠a se conoce como un claro oscuro. Trata de
mejorar las partes que tienen mayor relevancia.

\hypertarget{pooling}{%
\subsection{Pooling}\label{pooling}}

Con la operaci√≥n de pooling se trata de condensar la informaci√≥n de la
capa convolucional. A este procedimiento tambi√©n se le conoce como
submuestreo. Es simplemente una operaci√≥n en la que reducimos los
par√°metros de la red. Se aplica normalmente a trav√©s de dos operaciones:
max-pooling y mean-pooling, que tambi√©n es conocido como
average-pooling. Tal y como se observa en la imagen siguiente, desde la
capa de convoluci√≥n se genera una nueva capa aplicando la operaci√≥n a
todas las agrupaciones, donde previamente hemos elegido el tama√±o de la
regi√≥n; en la figura siguiente es de tama√±o 2, con lo que pasamos de un
espacio de 24 x 24 neuronas a la mitad, 12 x 12 en la capa de pooling.

Figura 71. etapa de pooling de tama√±o 2 x 2

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/pooling.png}

}

\caption{\label{fig-pooling}etapa de pooling de tama√±o 2 x 2}

\end{figure}

Vamos a estudiar el pooling suponiendo que tenemos una imagen de 5 x 5
p√≠xeles y que queremos efectuar una agrupaci√≥n max-pooling. Es la m√°s
utilizada, ya que obtiene buenos resultados. Observamos los valores de
la matriz y se escoge el valor m√°ximo de los cuatro bloques de matrices
de dos por dos. Max Pooling

En la agrupaci√≥n Average Pooling la operaci√≥n que se realiza es
sustituir los valores de cada grupo de entrada por su valor medio. Esta
transformaci√≥n es menos utilizada que el max-pooling.

La transformaci√≥n max-pooling presenta un tipo de invarianza local:
peque√±os cambios en una regi√≥n local no var√≠an el resultado final
realizado con el max -- pooling: se mantiene la relaci√≥n espacial. Para
ilustrar este concepto hemos escogido la imagen que presenta Torres
(2020) donde se ilustra como partiendo de una matriz de 12 x 12 que
representa al n√∫mero 7, al aplicar la operaci√≥n de max-pooling con una
ventana de 2 x 2 se conserva la relaci√≥n espacial.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/max_pooling.png}

}

\caption{\label{fig-max-pooling}Max Pooling}

\end{figure}

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/average_pooling.png}

}

\caption{\label{fig-average-pooling}Average Pooling}

\end{figure}

Fuente: Torres. J. (2020)

\hypertarget{padding}{%
\subsection{Padding}\label{padding}}

Para explicar el concepto del Padding vamos a suponer que tenemos una
imagen de 5 x 5 p√≠xeles, es decir 25 neuronas en la capa de entrada, y
que elegimos, para realizar la convoluci√≥n, una ventana de 3 x 3. El
n√∫mero de neuronas de la capa oculta resultar√° ser de nueve. Enumeramos
los p√≠xeles de la imagen de forma natural del 1 al 25 para que resulte
m√°s sencillo de entender.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/transformacion_pooling.png}

}

\caption{\label{fig-transformacion-pooling}mantenimiento del pooling con
la transformaci√≥n}

\end{figure}

Pero si queremos obtener un tensor de salida que tenga las mismas
dimensiones que la entrada podemos rellenar la matriz de ceros antes de
deslizar la ventana por ella. Vemos la figura siguiente donde ya se ha
rellenado de valores cero y obtenemos, despu√©s de deslizar la ventana de
3 x3 de izquierda a derecha y de arriba abajo, las veinticinco matrices
de la figura n¬∫ 71

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/sin_padding.png}

}

\caption{\label{fig-sin-padding}Operaci√≥n de convoluci√≥n con una ventana
de 3 x 3}

\end{figure}

Figura n¬∫ 74. imagen con relleno de ceros

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/relleno_ceros.png}

}

\caption{\label{fig-relleno-ceros}Imagen con relleno de ceros}

\end{figure}

Cuando utilizamos el programa keras disponemos de dos opciones para
llevar a cabo esta operaci√≥n de padding: ``same'' y ``valid''. Si
utilizamos ``valid'' implica no hacer padding y el m√©todo ``same''
obliga a que la salida tenga la misma dimensi√≥n que la entrada.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/con_padding.png}

}

\caption{\label{fig-con-padding}Operaci√≥n de convoluci√≥n con ventana 3 x
3 y padding}

\end{figure}

\hypertarget{stride}{%
\subsection{Stride}\label{stride}}

Hasta ahora, la forma de recorrer la matriz a trav√©s de la ventana se
realiza desplaz√°ndola de un solo paso, pero podemos cambiar este
hiperpar√°metro conocido como stride. Al aumentar el paso se decrementa
la informaci√≥n que pasar√° a la capa posterior. A continuaci√≥n, se
muestra el resultado de las cuatro matrices que obtenemos con un stride
de valor 3.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo1/con_stride2.png}

}

\caption{\label{fig-con-stride2}Operaci√≥n de convoluci√≥n con una ventana
de 3 x 3 y stride 2}

\end{figure}

Finalmente, para resumir, una red convolucional contiene los siguientes
elementos: ‚Ä¢ Entrada: Son el n√∫mero de pixeles de la imagen. Ser√°n alto,
ancho y profundidad. Tenemos un solo color (escala de grises) o tres:
rojo, verde y azul. ‚Ä¢ Capa de convoluci√≥n: procesar√° la salida de
neuronas que est√°n conectadas en ¬´regiones locales¬ª de entrada (es decir
pixeles cercanos), calculando el producto escalar entre sus pesos (valor
de pixel) y una peque√±a regi√≥n a la que est√°n conectados. En este
ep√≠grafe se presentan las im√°genes con 32 filtros, pero puede realizarse
con la cantidad que deseemos. ‚Ä¢ ¬´Capa RELU¬ª Se aplicar√° la funci√≥n de
activaci√≥n en los elementos de la matriz. ‚Ä¢ POOL (agrupar) o
Submuestreo: Se procede normalmente a una reducci√≥n en las dimensiones
alto y ancho, pero se mantiene la profundidad. ‚Ä¢ CAPA tradicional. Se
finalizar√° con la red de neuronas feedforward (Perceptr√≥n multicapa que
se denomina normalmente como red densamente conectada) que vincular√° con
la √∫ltima capa de subsampling y finalizar√° con la cantidad de neuronas
que queremos clasificar. En el gr√°fico siguiente se muestran todas las
fases de una red neuronal convolucional.

\includegraphics{imagenes/capitulo1/convolucion_completa.png} Fuente:
Raschka y Mirjalili (2019)

\hypertarget{redes-convolucionales-con-nombre-propio}{%
\subsection{Redes convolucionales con nombre
propio}\label{redes-convolucionales-con-nombre-propio}}

Existen en la actualidad muchas arquitecturas de redes neuronales
convolucionales que ya est√°n preparadas, probadas, disponibles e
incorporadas en el software de muchos programas como Keras y Tensorflow.
Vamos a comentar algunos de estos modelos, bien por ser los primeros, o
por sus excelentes resultados en concursos como el ILSVRC (Large Scale
Visual Recognition Challenge). Estas estructuras merecen atenci√≥n dado
que son excelentes para estudiarlas e incorporarlas por su notable
√©xito. El ILSVRC fue un concurso celebrado de 2011 a 2016 de donde
nacieron las principales aportaciones efectuadas en las redes
convolucionales. Este concurso fue dise√±ado para estimular la innovaci√≥n
en el campo de la visi√≥n computacional. Actualmente se desarrollan este
tipo de concursos a trav√©s de la plataforma web: https://www.kaggle.com/
Para ver m√°s prototipos de redes convolucionales y los √∫ltimos avances y
consejos sobre las redes convolucionales se puede consultar el siguiente
art√≠culo ``Recent Advances in Convolutional Neural Networks'' de
Jiuxiang. G. et al.~(2019) Los cinco modelos m√°s destacados hasta el a√±o
2017 son los siguientes: LeNet-5, Alexnet, GoogLeNet, VGG y Restnet. 1.
LeNet-5. Este modelo de Yann LeCun de los a√±os 90 consigui√≥ excelentes
resultados en la lectura de c√≥digos postales consta de im√°genes de
entrada de 32 x 32 p√≠xeles seguida de dos etapas de convoluci√≥n --
pooling, una capa densamente conectada y una capa softmax final que nos
permite conocer los n√∫meros o las im√°genes. 2. AlexNet. Fue la
arquitectura estrella a partir del a√±o 2010 en el ILSVRC y popularizada
en el documento de 2012 de Alex Krizhevsky, et
al.~titulado''Clasificaci√≥n de ImageNet con redes neuronales
convolucionales profundas''. Podemos resumir los aspectos clave de la
arquitectura relevantes en los modelos modernos de la siguiente manera:
‚Ä¢ Empleo de la funci√≥n de activaci√≥n ReLU despu√©s de capas
convolucionales y softmax para la capa de salida. ‚Ä¢ Uso de la agrupaci√≥n
m√°xima en lugar de la agrupaci√≥n media. ‚Ä¢ Utilizaci√≥n de la
regularizaci√≥n de Dropout entre las capas totalmente conectadas. ‚Ä¢
Patr√≥n de capa convolucional alimentada directamente a otra capa
convolucional. ‚Ä¢ Uso del aumento de datos (Data Aumentation,) 3. VGG.
Este prototipo fue desarrollado por un grupo de investigaci√≥n de
Geometr√≠a Visual en Oxford. Obtuvo el segundo puesto en la competici√≥n
del a√±o 2014 del ILSVRC. Las aportaciones principales de la
investigaci√≥n se pueden encontrar en el documento titulado ``~Redes
convolucionales muy profundas para el reconocimiento de im√°genes a gran
escala~'' desarrollado por Karen Simonyan y Andrew Zisserman. Este
modelo contribuy√≥ a demostrar que la profundidad de la red es una
componente cr√≠tica para alcanzar unos buenos resultados. Otra diferencia
importante con los modelos anteriores y que actualmente es muy utilizada
es el uso de un gran n√∫mero de filtros y de tama√±o reducido. Estas redes
emplean ejemplos de dos, tres e incluso cuatro capas convolucionales
apiladas antes de usar una capa de agrupaci√≥n m√°xima. En esta
arquitectura el n√∫mero de filtros aumenta con la profundidad del modelo.
El modelo comienza con 64 y aumenta a trav√©s de los filtros de 128, 256
y 512 al final de la parte de extracci√≥n de caracter√≠sticas del modelo.
Los investigadores evaluaron varias variantes de la arquitectura si bien
en los programas s√≥lo se hace referencia a dos de ellas que son las que
aportan un mayor rendimiento y que son nombradas por las capas que
tienen: VGG-16 y VGG-19. 4. GoogLeNet. GoogLeNet fue desarrolado por
investigadores de Google Research. de Google, que con su m√≥dulo
denominado de inception reduce dr√°sticamente los par√°metros de la red
(10 veces menos que AlexNet) y de ella han derivado varias versiones
como la Inception-v4. Esta arquitectura gan√≥ la competici√≥n en el a√±o
2014 y su √©xito se debi√≥ a que la red era mucho m√°s profunda (muchas m√°s
capas) y como ya se ha indicado introdujeron en el modelo las subredes
llamadas inception. Las aportaciones principales en el uso de capas
convolucionales fueron propuestos en el documento de 2015 por Christian
Szegedy, et al.~titulado ``~Profundizando con las convoluciones~''.
Estos autores introducen una arquitectura llamada ``inicio'' y un modelo
espec√≠fico denominado GoogLenet. El m√≥dulo inicio es un bloque de capas
convolucionales paralelas con filtros de diferentes tama√±os y una capa
de agrupaci√≥n m√°xima de 3 √ó 3, cuyos resultados se concatenan. Otra
decisi√≥n de dise√±o fundamental en el modelo inicial fue la conexi√≥n de
la salida en diferentes puntos del modelo que lograron realizar con la
creaci√≥n de peque√±as redes de salida desde la red principal y que fueron
entrenadas para hacer una predicci√≥n. La intenci√≥n era proporcionar una
se√±al de error adicional de la tarea de clasificaci√≥n en diferentes
puntos del modelo profundo para abordar el problema de los gradientes de
fuga. 5. Red Residual o ResNet. Esta arquitectura gano la competici√≥n de
2015 y fue creada por el grupo de investigaci√≥n de Microsoft. Se puede
ampliar la informaci√≥n en He, et al.~en su documento de 2016 titulado
``~Aprendizaje profundo residual para el reconocimiento de la imagen~''.
Esta red es extremadamente profunda con 152 capas, confirmando al pasar
los a√±os que las redes son cada vez m√°s profundas, m√°s capas, pero con
menos par√°metros que estimar. La cuesti√≥n clave del dise√±o de esta red
es la incorporaci√≥n de la idea de bloques residuales que hacen uso de
conexiones directa. Un bloque residual, seg√∫n los autores, ``es un
patr√≥n de dos capas convolucionales con activaci√≥n ReLU donde la salida
del bloque se combina con la entrada al bloque, por ejemplo, la conexi√≥n
de acceso directo'' Otra clave, en este caso para el entrenamiento de la
red tan profunda es lo que llamaron skip connections que implica que la
se√±al con la que se alimenta una capa tambi√©n se agregue a una capa que
se encuentre m√°s adelante. Resumiendo, las tres principales aportaciones
de este modelo son: ‚Ä¢ Empleo de conexiones de acceso directo. ‚Ä¢
Desarrollo y repetici√≥n de los bloques residuales. ‚Ä¢ Modelos muy
profundos (152 capas) Aunque se encuentran otros modelos que tambi√©n son
muy populares con 34, 50 y 101 capas. Una buena parte de los modelos
comentados se incluyen en la librer√≠a de Keras y se pueden encontrar en
la siguiente direcci√≥n de internet: https://keras.io/api/applications/
Seg√∫n los autores del programa Keras: ``Las aplicaciones Keras son
modelos de aprendizaje profundo que est√°n disponibles junto con pesos
preentrenados. Estos modelos se pueden usar para predicci√≥n, extracci√≥n
de caracter√≠sticas y ajustes. Los pesos se descargan autom√°ticamente
cuando se crea una instancia de un modelo. Se almacenan en
\textasciitilde{} / .keras / models /. Tras la creaci√≥n de instancias,
los modelos se construir√°n de acuerdo con el formato de datos de imagen
establecido en su archivo de configuraci√≥n de Keras en \textasciitilde{}
/ .keras / keras.json. Por ejemplo, si ha configurado
image\_data\_format = channel\_last, cualquier modelo cargado desde este
repositorio se construir√° de acuerdo con la convenci√≥n de formato de
datos TensorFlow,''Altura-Ancho-Profundidad''.

\includegraphics{imagenes/capitulo1/modelos_entrenados.png} Fuente :
https://keras.io/api/applications/

\hypertarget{redes-nueronales-recurrentes}{%
\section{Redes Nueronales
Recurrentes}\label{redes-nueronales-recurrentes}}

\hypertarget{autoencoders}{%
\section{Autoencoders}\label{autoencoders}}

\textbf{Bases del Autoencoder} Los Autoencoders (AE) son uno de los
tipos de redes neuronales que caen dentro del √°mbito del Deep Learning,
en la que nos encontramos con un modelo de aprendizaje no supervisado.
Ya se empez√≥ a hablar de AE en la d√©cada de los 80 (Bourlard and Kamp
1988), aunque es en estos √∫ltimos a√±os donde m√°s se est√° trabajando con
ellos. La arquitectura de un AE es una Red Neuronal Artificial (ANN por
sus siglas en ingl√©s) que se encuentra dividida en dos partes, encoder y
decoder (Charte et al.~2018), (Goodfellow, Bengio, and Courville 2016).
El encoder va a ser la parte de la ANN que va codificar o comprimir los
datos de entrada, y el decoder ser√° el encargado de regenerar de nuevo
los datos en la salida. Esta estructura de codificaci√≥n y decodificaci√≥n
le llevar√° a tener una estructura sim√©trica. El AE es entrenado para ser
capaz de reconstruir los datos de entrada en la capa de salida de la
ANN, implementando una serie de restricciones (la reducci√≥n de elementos
en las capas ocultas del encoder) que van a evitar que simplemente se
copie la entrada en la salida. Si recordamos la estructura de una ANN
cl√°sica o tambi√©n llamada Red Neuronal Densamente Conectada (ya que cada
neurona conecta con todas las de la siguiente capa) nos encontramos en
que en esta arquitectura, generalmente, el n√∫mero de neuronas por capa
se va reduciendo hasta llegar a la capa de salida que deber√≠a ser
normalmente un n√∫mero (si estamos en un problema regresi√≥n), un dato
binario (si es un problema de clasificaci√≥n). Figura n¬∫ 86: Red Neuronal
Clasica

Fuente: Elaboraci√≥n propia Si pensamos en una estructura b√°sica de AE en
la que tenemos una capa de entrada, una capa oculta y una capa de
salida, √©sta ser√≠a su representaci√≥n:

Figura n¬∫ 87: Autoencoder b√°sico

Fuente: Elaboraci√≥n propia Donde los valores de son los datos de entrada
y los datos son la reconstrucci√≥n de los mismos despu√©s de pasar por la
capa oculta que tiene s√≥lo dos dimensiones. El objetivo del
entrenamiento de un AE ser√° que estos valores de sean lo m√°s parecidos
posibles a los . Seg√∫n (Charte et al.~2018) los AE se puden clasificar
seg√∫n el tipo de arquitectura de red en: ‚Ä¢ Incompleto simple ‚Ä¢
Incompleto profundo ‚Ä¢ Extra dimensionado simple ‚Ä¢ Extra dimensionado
profundo

Figura n¬∫ 88: Tipos de Autoencoders por arquitectura

Fuente: Elaboraci√≥n propia Cuando hablamos de Incompleto nos referimos a
que tenemos una reducci√≥n de dimensiones que permite llegar a conseguir
una ``compresi√≥n'' de los datos iniciales como t√©cnica para que aprenda
los patrones internos. En el caso de Extra dimensionado es cuando
subimos de dimensi√≥n para conseguir que aprenda esos patrones. En este
√∫ltimo caso ser√≠a necesario aplicar t√©cnicas de regularizaci√≥n para
evitar que haya un sobreajuste en el aprendizaje. Cuando hablamos de
Simple estamos haciendo referencia a que hay una √∫nica capa oculta, y en
el caso de Profundo es que contamos con m√°s de una capa oculta.
Normalmente se trabaja con las arquitecturas de tipo Incompleto
profundo, sobre todo cuando se est√° trabajando con tipos de datos que
son im√°genes. Aunque tambi√©n podr√≠amos encontrar una combinaci√≥n de
Incompleto con Extra dimensionado profundo cuando trabajamos con tipos
de datos que no son im√°genes y as√≠ crecer en la primera o segunda capa
oculta, para luego reducir. Esto nos permitir√≠a por ejemplo adaptarnos a
estructuras de AE en las que trabajemos con n√∫mero de neuronas en una
capa que sean potencia de 2, y poder construir arquitecturas din√°micas
en funci√≥n del tama√±o de los datos, adapt√°ndolos a un tama√±o prefijado.
A continuaci√≥n, vemos un gr√°fico de una estructura mixta Extra
dimensionado - Incompleto profundo. Figura n¬∫ 89: Autoencoder Mixto
(Incompleto y Extra dimensionado)

Fuente: Elaboraci√≥n propia Idea intuitiva del uso de Autoencoders Si un
AE trata de reproducir los datos de entrada mediante un encoder y
decoder, ¬øque nos puede aportar si ya tenemos los datos de entrada? Ya
hemos comentado que la red neuronal de un AE es sim√©trica y est√° formada
por un encoder y un decoder, adem√°s cuando trabajamos con los AE que son
incompletos, se est√° produciendo una reducci√≥n del tama√±o de los datos
en la fase de codificaci√≥n y de nuevo una regeneraci√≥n a partir de esos
datos m√°s peque√±os al original. Ya tenemos uno de los conceptos m√°s
importantes de los AE que es la reducci√≥n de dimensiones de los datos de
entrada. Estas nuevas variables que se generan una vez pasado el encoder
se les suele llamar el espacio lantente. Este concepto de reducci√≥n de
dimensiones en el mundo de la miner√≠a de datos lo podemos asimiliar
r√°pidamente a t√©cnicas como el An√°lisis de Componentes Principales
(PCA), que nos permite trabajar con un n√∫mero m√°s reducido de
dimensiones que las originales. Igualmente, esa reducci√≥n de los datos y
la capacidad de poder reconstruir el original podemos asociarlo al
concepto de compresi√≥n de datos, de forma que con el encoder podemos
comprimir los datos y con el decoder los podemos descomprimir. En este
caso habr√≠a que tener en cuenta que ser√≠a una t√©cnica de compresi√≥n de
datos con p√©rdida de informaci√≥n (JPG tambi√©n es un formato de
compresi√≥n con p√©rdida de compresi√≥n). Es decir, con los datos
codificados y el AE (pesos de la red neuronal), ser√≠amos capaces de
volver a regenerar los datos originales. Otra de las ideas alrededor de
los AE es que, si nosotros tenemos un conjunto de datos de la misma
naturaleza y los entrenamos con nuestro AE, somos capaces de construir
una red neuronal (pesos en la red neuronal) que es capaz de reproducir
esos datos a trav√©s del AE. Que ocurre si nosotros metemos un dato que
no era de la misma naturaleza que los que entrenaron el AE, lo que
tendremos entonces es que al recrear los datos originales no va a ser
posible que se parezca a los datos de entrada. De forma que el error que
vamos a tener va a ser mucho mayor por no ser datos de la misma
naturaleza. Esto nos puede llevar a construir un AE que permita detectar
anomal√≠as, es decir, que seamos capaces de detectar cuando un dato es
una anomal√≠a porque realmente el AE no consigue tener un error lo
bastante peque√±o. Seg√∫n lo visto de forma intuitiva vamos a tener el
encoder que ser√° el encargado de codificar los datos de entrada y luego
tendremos el decoder que ser√° el encargado de realizar la decodificaci√≥n
y conseguir acercarnos al dato original . Es decir intentamos conseguir
. Si suponemos un Simple Autoencoder en el que tenemos una √∫nica capa
oculta, con una funci√≥n de activaci√≥n intermedia y una funci√≥n de
activaci√≥n de salida y los par√°metros y represetan los par√°metros de la
red neuronal en cada capa, tendr√≠amos la siguiente expresi√≥n: {[}190{]}
{[}191{]}

As√≠ tendremos que donde ser√° la reconstrucci√≥n de Una vez tenemos la
idea intuitiva de para qu√© nos puede ayudar un AE, recopilamos algunos
de los principales usos sobre los que actualmente se est√° trabajando.
M√°s adelante ,comentaremos algunos de ellos con m√°s detalle. Principales
Usos de los Autoencoders A continuaci√≥n, veamos la explicaci√≥n de cuales
son algunos de los principales usos de los autoencoders: Reducci√≥n de
dimensiones / Compresi√≥n de datos En la idea intuitiva de los AE ya
hemos visto claro que se pueden usar para la reducci√≥n de dimensiones de
los datos de entrada. Si estamos ante unos datos de entrada de tipo
estructurado estamos en un caso de reducci√≥n de dimensiones cl√°sico, en
el que queremos disminuir el n√∫mero de variables con las que trabajar.
Muchas veces este tipo de trabajo se hace mediante el PCA (An√°lisis de
Componente Principales, por sus siglas en ingl√©s), sabiendo que lo que
se realiza es una transformaci√≥n lineal de los datos, ya que conseguimos
unas nuevas variables que son una combinaci√≥n lineal de las mismas. En
el caso de los AE conseguimos mediante las funciones de activaci√≥n no
lineales (simgmoide, ReLu, tanh, etc) combinaciones no lineales de las
variables originales para reducir las dimensiones. Tambi√©n existen
versiones de PCA no lineales llamadas Kernel PCA que mediante las
t√©cnicas de kernel son capaces de construir relaciones no lineales. En
esta l√≠nea estamos viendo que cuando el encoder ha actuado, tenemos unos
nuevos datos m√°s reducidos y que somos capaces de practicamente volver a
reproducir teniendo el decoder. Podr√≠amos pensar en este tipo de t√©cnica
para simplemente comprimir informaci√≥n. Hay que tener en cuenta que este
tipo de t√©cnicas no se pueden aplicar a cualquier dato que queramos
comprimir, ya que debemos haber entrenado al AE con unos datos de
entrenamiento que ha sido capaz de obtener ciertos patrones de ellos, y
por eso es capaz luego de reproducirlos. B√∫squeda de im√°genes Cuando
pensamos en un buscador de im√°genes nos podemos hacer a la idea que el
buscar al igual que con el texto nos va a mostrar entradas que se√°n
im√°genes parecidas a la que estamos buscando. Si construimos un
autoencoder, el encoder nos va a dar unas variables con informaci√≥n para
poder recrear de nuevo la imagen. Lo que parece claro es que si hay muy
poca distancia entre estas variables y otras la reconstrucci√≥n de la
imagen ser√° muy parecida. As√≠ nosotros podemos entrenar el AE con
nuestro conjunto de im√°genes, una vez tenemos el AE pasamos el encoder a
todas las im√°genes y las tenemos todas en ese nuevo espacio de
variables. Cuando queremos buscar una imagen, le pasamos el autoencoder,
y ya buscamos las m√°s cercanas a nuestra imagen en el espacio de
variables generado por el encoder. Detecci√≥n de Anomal√≠as Cuando estamos
ante un problema de clasificaci√≥n y tenemos un conjunto de datos que
est√° muy desbalanceado, es decir, tenemos una clase mayoritaria que es
mucho m√°s grande que la minoritaria (posiblemente del orden de m√°s del
95\%), muchas veces es complicado conseguir un conjunto de datos
balanceado que sea realmente bueno para hacer las predicciones. Cuando
estamos en estos entornos tan desbalanceados muchas veces se dice que
estamos ante un sistema para detectar anomal√≠as. Un AE nos puede ayudar
a detectar estas anomal√≠as de la siguiente forma: ‚Ä¢ Tomamos todos los
datos de entrenamiento de la clase mayoritaria (o normales) y
construimos un AE para ser capaces de reproducirlos. Al ser todos estos
datos de la misma naturaleza conseguiremos entrenar el AE con un error
muy peque√±o. ‚Ä¢ Ahora tomamos los datos de la clase minoritaria (o a
nomal√≠as) y los pasamos a trav√©s del AE obteniendo unos errores de
reconstrucci√≥n. ‚Ä¢ Definimos el umbral de error que nos separar√° los
datos normales de las anomal√≠as, ya que el AE s√≥lo est√° entrenado con
los normales y conseguir√° un error m√°s alto con las anomal√≠as al
reconstruirlas. ‚Ä¢ Cogemos los datos de test y los vamos pasando por el
AE, si el error es menor del umbral, entonces ser√° de la clase
mayoritaria. Si el error es mayor que el umbral, entonces estaremos ante
una anomal√≠a. Eliminaci√≥n de ruido Otra de las formas de uso de los
autoencoders en tratamiento de im√°genes es para eliminar ruido de las
mismas, es decir poder quitar manchas de las im√°genes. La forma de hacer
esto es la siguiente: ‚Ä¢ Partimos de un conjunto de datos de
entrenamiento (im√°genes) a las que le metemos ruido, por ejemplo,
modificando los valores de cada pixel usando una distribuci√≥n normal, de
forma que obtenemos unos datos de entrenamiento con ruido. ‚Ä¢ Construimos
el AE de forma que los datos de entrada son los que tienen ruido, pero
los de salida vamos a forzar que sean los originales. De forma que
intentamos que aprendan a reconstruirse como los que no tienen ruido. ‚Ä¢
Una vez que tenemos el AE y le pasamos datos de test con ruido, seremos
capaces de reconstruirlos sin el ruido. Modelos generativos Cuando
hablamos de modelos generativos, nos referimos a AE que son capaces de
generar cosas nuevas a las que exist√≠an. De forma que mediante t√©cnicas
como los Variational Autoencoders, los Adversarial Autoencoders seremos
capaces de generar nuevas im√°genes que no ten√≠amos inicialmente. Es
decir, podr√≠amos pensar en poder tener un AE que sea capaz de
reconstruir im√°genes de caras, pero que adem√°s con toda la informaci√≥n
aprendida fuera capaz de generar nuevas caras que realmente no existen.
Dise√±o del modelo de AE Transformaci√≥n de datos Cuando se trabaja con
redes neuronales y en particular con AEs, necesitamos representar los
valores de las variables de entrada en forma num√©rica. En una red
neuronal todos los datos son siempre num√©ricos. Esto significa que todas
aquellas variables que sean categ√≥ricas necesitamos convertirlas en
num√©ricas. Adem√°s es muy conveniente normalizar los datos para poder
trabajar con valores entre 0 y 1, que van a ayudar a que sea m√°s f√°cil
que se pueda converger a la soluci√≥n. Como ya sabemos normalmente nos
encontramos que en una red neuronal las variables de salida son: ‚Ä¢ un
n√∫mero (regresi√≥n) ‚Ä¢ una serie de n√∫meros (regresi√≥n m√∫ltiple) ‚Ä¢ un dato
binario (clasificaci√≥n binaria) ‚Ä¢ un n√∫mero que representa una categor√≠a
(clasifiaci√≥n m√∫ltiple) En el caso de los AE puede que tengamos una gran
parte de las veces valores de series de n√∫meros, ya que necesitamos
volver a representar los datos de entrada. Esto significa que tendremos
que conseguir en la capa de salida esos datos num√©ricos que ten√≠amos
inicialmente, como si se tuviera una regresi√≥n m√∫ltiple. Arquitectura de
red Como ya se ha comentado en las redes neuronales, algunos de los
hiperpar√°metros m√°s importantes en un AE son los relacionados con la
arquitectura de la red neuronal. Para la construcci√≥n de un AE vamos a
elegir una topolog√≠a sim√©trica del encoder y el decoder. Durante el
dise√±o del AE necesitaremos ir probando y adaptando todos estos
hiperpar√°metros de la ANN para conseguir que sea lo m√°s eficiente
posible: ‚Ä¢ N√∫mero de capas ocultas y neuronas en cada una ‚Ä¢ Funci√≥n de
coste y p√©rdida ‚Ä¢ Optimizador ‚Ä¢ Funci√≥n de activaci√≥n en capas ocultas ‚Ä¢
Funci√≥n de activaci√≥n en salida N√∫mero de capas ocultas y neuronas en
cada una La selecci√≥n del n√∫mero de capas ocultas y la cantidad de
neuronas en cada una va a ser un procedimiento de prueba y error en el
que se pueden probar muchas combinaciones. Es cierto que en el caso de
trabajar con im√°genes y CNN ya hay muchas arquitecturas definidas y
probadas que consiguen muy buenos resultados. Por otro lado para tipos
de datos estructurados ser√° muy dependiente de esos datos, de forma que
ser√° necesario realizar diferentes pruebas para conseguir un buen
resultado. Funci√≥n de coste y p√©rdida En este caso no hay ninguna
recomendaci√≥n especial para las funciones de costes/p√©rdida y depender√°
al igual que en las redes neuronales de la naturaleza de los datos de
salida con los que vamos a trabajar. Optimizador Se recomienda usar el
optimizador ADAM (Diederik P. Kingma 2017) que es el que mejores
resultados ha dado en las pruebas seg√∫n (Walia 2017), consiguiendo una
convergencia m√°s r√°pida que con el resto de optimizadores.

Funci√≥n de activaci√≥n en capas ocultas En un AE las funciones de
activaci√≥n en las capas ocultas van a conseguir establecer las
restricciones no lineales al pasar de una capa a la siguiente,
normalmente se evita usar la funci√≥n de activaci√≥n lineal en las capas
intermedias ya que queremos conseguir transformaciones no lineales. Se
recomienda usar la funci√≥n de activaci√≥n ReLu en las capas ocultas, ya
que parece ser que es la que mejores resultados da en la convergencia de
la soluci√≥n y adem√°s menor coste computacional tiene a la hora de
realizar los c√°lculos. Funci√≥n de activaci√≥n en salida En la capa de
salida tenemos que tener en cuenta cual es el tipo de datos final que
queremos obtener, que en el caso de un AE es el mismo que el tipo de
dato de entrada. Normalmente las funciones de activaci√≥n que se usar√°n
en la √∫ltima capa seran: ‚Ä¢ Lineal con multiples unidades, para regresi√≥n
de varios datos num√©ricos ‚Ä¢ Sigmoid para valores entre 0 y 1 Tipos de
Autoencoders Una vez entendido el funcionamiento de los AE, veamos
algunos de los AE que se pueden construir para diversas tareas. ‚Ä¢ Simple
‚Ä¢ Multicapa o Profundo ‚Ä¢ Sparse ‚Ä¢ Convolucional ‚Ä¢ Denoising ‚Ä¢
Variational En la descripci√≥n de los tipos de AE vamos usar c√≥digo en R
y en python y el framework keras con el backend Tensorflow. Todo el
c√≥digo se proporciona aparte. Usaremos como dataset a MINIST, que
contiene 60.000/10.000 (entrenamiento/validaci√≥n) im√°genes de los
n√∫meros del 0 al 9, escritos a mano. Cada imagen tiene un tama√±o de
28x28 = 784 pixels, en escala de grises, con lo que para cada pixel
tendremos un valor entre 0 y 255 para definir cu√°l es su intensidad de
gris. Autoencoder Simple Vamos a describir como construir un autoencoder
Simple usando una red neuronal densamente conectada en lugar de usar una
red neuronal convolucional, para que sea m√°s sencillo comprender el
ejemplo. Es decir, vamos a tratar los datos de entrada como si fueran
unos datos num√©ricos que queremos reproducir y no vamos a utilizar
ninguna de las t√©cnicas asociadas a las redes convolucionales. Hay que
recordar que las redes convolucionales permiten mediante un tratamiento
de las im√°genes (convoluci√≥n, pooling, etc) conseguir mejores resultados
que si lo hici√©ramos directamente con redes densamente conectadas. En
este caso tendremos una capa de entrada con 784 neuronas
(correspondientes a los pixels de cada imagen), una capa intermedia de
32 neuronas, y una capa de salida de nuevo de las 784 neuronas para
poder volver a obtener de nuevo los datos originales. En nuestro ejemplo
vamos a tener los siguientes elementos. ‚Ä¢ 1 capa de entrada (784 datos),
1 capa oculta (32 datos) y una capa de salida (784 datos) ‚Ä¢ La funci√≥n
de coste/p√©rdida va a ser la Entrop√≠a ‚Ä¢ Usaremos el optimizador Adam ‚Ä¢
Como funci√≥n activaci√≥n intermedia usaremos ReLu ‚Ä¢ Como funci√≥n
activaci√≥n de salida sigmoid (ya que queremos un valor entre 0 y 1 )
Autoencoder Sparse Ya hemos comentado que una forma de conseguir que un
autoencoder aprenda estructuras o correlaciones es la reducci√≥n del
n√∫mero de neuronas, pero parte de este trabajo tambi√©n se puede
conseguir mediante t√©cnicas de sparsing (escasez). Este tipo de t√©cnicas
se usan normalmente en las ANN para evitar el sobreajuste de nuestro
modelo, de forma que en cada actualizaci√≥n de los pesos de la red no se
tienen en cuenta todas las neuronas de la capa. Es decir, vamos a
conseguir que en las capas que decidamos no todas las neuronas van a
estar activadas, de esta manera adem√°s de ayudar a evitar el
sobreajuste, tambi√©n conseguiremos crear esas correlaciones que ayudan a
construir el autoencoder. Existen dos metodos b√°sicos para generar el
sparse que son: ‚Ä¢ Regularizaci√≥n L1 ‚Ä¢ Regularizaci√≥n L2 B√°sicamente los
dos m√©todos tratan de hacer que los pesos de las neuronas tengan valores
muy peque√±os consiguiendo una distribuci√≥n de pesos m√°s regular. Esto lo
consiguen al a√±adir a la funci√≥n de p√©rdida un coste asociado a tener
pesos grandes en las neuronas. Este peso se puede construir o bien con
la norma L1 (proporcional al valor absoluto) o con la norma L2
(proporcional al cuadrado de los coeficientes de los pesos). B√°sicamente
trabajaremos con un AE Simple, con una red densamente conectada, al que
le aplicaremos la regularizaci√≥n en su capa oculta. En nuestro ejemplo
vamos a tener los siguientes elementos. - 1 capa de entrada, 1 capaa
oculta y una capa de salida - Las capas ocultas tendr√°n aplicada la
Regularizaci√≥n L2 - La funci√≥n de coste/p√©rdida va a ser la Entrop√≠a -
Usaremos el optimizador Adam - Como funci√≥n activaci√≥n intermedia
usaremos ReLu - Como funci√≥n activaci√≥n de salida sigmoid (ya que
queremos un valor entre 0 y 1) Autoencoder Multicapa o profundo Vamos a
pasar ahora a una versi√≥n del autoencoder donde habilitamos m√°s capas
ocultas y hacemos que el descenso del n√∫mero de neuronas sea m√°s gradual
hasta llegar a nuestro valor deseado, para luego volver a reconstruirlo.
En este caso seguimos con redes densamente conectadas y aplicamos varias
capas intermedias reduciendo el n√∫mero de neuronas en cada una hasta
llegar a la capa donde acaba el encoder para volver a ir creciendo en
las sucesivas capas hasta llegar a la de salida. En nuestro ejemplo
vamos a tener los siguientes elementos. - 1 capa de entrada (784 datos),
5 capa ocultas (32 datos intermedia) y una capa de salida (784 datos) -
La funci√≥n de coste/p√©rdida va a ser la Entrop√≠a - Usaremos el
optimizador Adam - Como funci√≥n activaci√≥n intermedia usaremos ReLu -
Como funci√≥n activaci√≥n de salida sigmoid (ya que queremos un valor
entre 0 y 1) Autoencoder Convolucional En nuestro ejemplo al estar
trabajando con im√°genes podemos pasar a trabajar con Redes
Convolucionales (CNN) de forma que en lugar de usar las capas densamente
conectadas que hemos usado hasta ahora, vamos a pasar a usar las
capacidades de las redes convolucionales. Al trabajar con redes
convolucionales necesitaremos trabajar con capas de convoluci√≥n o
pooling para llegar a la capa donde acaba el encoder para volver a ir
creciendo aplicando operaciones de convoluci√≥n y upsampling (contrario
al pooling). En nuestro ejemplo vamos a tener los siguientes elementos.
- 1 capa de entrada, 5 capas ocultas y una capa de salida - La funci√≥n
de coste/p√©rdida va a ser la Entrop√≠a - Usaremos el optimizador Adam -
Como funci√≥n activaci√≥n intermedia usaremos ReLu - Como funci√≥n
activaci√≥n de salida sigmoid (ya que queremos un valor entre 0 y 1)
Autoencoder Denoising Vamos a usar ahora un autoencoder para hacer
limpieza en imagen, es decir, conseguir a partir de una imagen que tiene
ruido otra imagen sin ese ruido. Entrenaremos al autoencoder para que
limpie ``ruido'' que hay en la imagen y lo reconstruya sin ello. El
ruido lo vamos a generar mediante una distribuci√≥n normal y
modificaremos el valor de los pixels de las im√°genes. Usaremos estas
im√°genes con ruido para que sea capaz de reconstruir la imagen original
sin ruido con el AE. Para realizar este proceso lo que haremos ser√°\_ ‚Ä¢
Crear nuevas im√°genes con ruido ‚Ä¢ Entrenar el autoencoder con estas
nuevas im√°genes ‚Ä¢ Calcular el error de reconstrucci√≥n respecto a las
im√°genes originales Al estar trabajando con im√°genes vamos a partir del
Autoencoder de Convoluci√≥n para poder aplicar el denosing. En nuestro
ejemplo vamos a tener los siguientes elementos. - 1 capa de entrada, 5
capas ocultas y una capa de salida - La funci√≥n de coste/p√©rdida va a
ser la Entrop√≠a - Usaremos el optimizador Adam - Como funci√≥n activaci√≥n
intermedia usaremos ReLu - Como funci√≥n activaci√≥n de salida sigmoid (ya
que queremos un valor entre 0 y 1) Autoencoder Variational Los
Variational Autoencoder son un tipo de modelo que se denomina
generativo, ya que va a permitir construir nuevas im√°genes que no
exist√≠an a partir de otras imagenes con las que se ha entrenado a la red
neuronal. En realidad, es un autoencoder que durante el entrenamiento se
le regulariza para evitar un sobreajuste y asegurar que en el espacio
latente (intermedio) tenga buenas propiedades que permitan un buen
proceso generativo. El proceso de construir este tipo de AE es muy
parecido a los que ya hemos visto, con una peque√±a diferencia en el paso
entre el proceso de encoder y el posterior decoder. Hasta ahora lo que
ten√≠amos era que lo que obten√≠amos del encoder se lo pas√°bamos
directamente al decoder, en este caso, el resultado del encoder no va a
ser realmente un dato, sino una distribuci√≥n de datos. De forma que al
decoder no se le pasa directamente lo que ha salido del encoder, si no
otro elemento cogido de la distribuci√≥n generada. En este caso el
proceso ser√≠a: ‚Ä¢ Se codifica la entrada con el encoder no como un dato
concreto, sino como una distribuci√≥n normal (media y desviaci√≥n) ‚Ä¢ Se
toma una muestra de un punto del espacio latente a partir de la
distribuci√≥n ‚Ä¢ Se decodifica el punto de muestra con el decoder ‚Ä¢ Se
calcula la funci√≥n de p√©rdida con el error de reconstrucci√≥n y la parte
de regularizaci√≥n ‚Ä¢ Se usa el backpropagation a trav√©s de la red
neuronal para ajustar los pesos Figura n¬∫ 90: Autoencoder Mixto
(Incompleto y Extra dimensionado)

Fuente:
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
Una vez que tenemos entrenado nuestro autoencoder seremos capaces de
construir nuevas im√°genes partiendo de puntos que est√©n en la
distribuci√≥n del espacio latente, de forma que esas peque√±as variaciones
van a dar lugar a im√°genes finales diferentes.

\hypertarget{arquiecturas-preentranadas}{%
\section{Arquiecturas Preentranadas}\label{arquiecturas-preentranadas}}

\hypertarget{detecciuxf3n-de-objetos}{%
\subsection{Detecci√≥n de objetos}\label{detecciuxf3n-de-objetos}}

\hypertarget{tratamiento-de-audios}{%
\subsection{Tratamiento de audios}\label{tratamiento-de-audios}}

\hypertarget{tareas-sobre-textos}{%
\subsection{Tareas sobre textos}\label{tareas-sobre-textos}}

\hypertarget{aprendizaje-por-refuerzo}{%
\section{Aprendizaje por Refuerzo}\label{aprendizaje-por-refuerzo}}

\hypertarget{introducciuxf3n-3}{%
\subsection{Introducci√≥n}\label{introducciuxf3n-3}}

Hasta ahora hemos visto como el Deep Learning se usa para el
\textbf{aprendizaje supervisado} y el \textbf{aprendizaje no
supervisado}, pero vamos a dar un paso m√°s, en el que veremos como usar
Deep Learning en otro tipo de aprendizaje llamado \textbf{aprendizaje
por refuerzo}.

El \textbf{Aprendizaje por Refuerzo} (\textbf{RL} por sus siglas en
ingles, Reinforcement Learning) trata de conseguir que el sistema
aprenda mediante recompensa/castigo, en funci√≥n de si los pasos que da
son buenos o malos. De esta manera, cuanta mayor recompensa se tenga es
que nuestro sistema se ha acercado a la soluci√≥n buena. Se trata de
aprender mediante la interacci√≥n y la retroalimentaci√≥n de lo que
ocurra.

Partiremos de dos elementos clave \textbf{agente} (es el que aprende y
toma decisiones), y el \textbf{entorno} (donde el agente aprende y
decide que acciones tomar). Tendremos que el agente podr√° realizar
\textbf{acciones} que normalmente provocar√°n un cambio de
\textbf{estado} y a la vez se tendr√° una \textbf{recompensa} (positiva o
negativa) en funci√≥n de la acci√≥n tomada en el entorno en ese momento.

Es decir, nos encontraremos un agente que realizar√° una acci√≥n \(a_t\)
en el tiempo \(t\), esta acci√≥n afectar√° al entorno que estar√° en un
estado \(S_t\) y mediante esta acci√≥n cambiar√° a un estado \(S_{t+1}\) y
adem√°s dar√° una recompensa \(r_{t+1}\) en funci√≥n de los malo o bueno
que haya sido este paso.

El agente volver√° a examinar el nuevo estado del entorno \(S_{t+1}\) y
la nueva recompensa recibida \(r_{t+1}\) y volver√° a tomar la decisi√≥n
de realizar una nueva acci√≥n \(a_{t+1}\).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl-diagrama.png}

}

\caption{Esquema Aprendizaje por Refuerzo - Fuente: Propia}

\end{figure}

\hypertarget{historia-del-aprendizaje-por-refuerzo}{%
\subsection{Historia del Aprendizaje por
Refuerzo}\label{historia-del-aprendizaje-por-refuerzo}}

\hypertarget{formalismo-matemuxe1tico}{%
\subsection{Formalismo Matem√°tico}\label{formalismo-matemuxe1tico}}

El formalismo matem√°tico para el Aprendizaje por Refuerzo est√° basado en
los \textbf{Procesos de Decisi√≥n de Markov} (MDP por sus siglas en
ingles). (CS229 Lecture notes).

\hypertarget{propiedad-de-markov}{%
\subsubsection{Propiedad de Markov}\label{propiedad-de-markov}}

Si tenemos una secuencia de estados \(s_1, s_2, ..., s_t\) y tenemos la
probabilidad de pasar a otro estado \(s_{t+1}\), diremos que se cumple
la \textbf{Propiedad de Markov} si el \textbf{futuro} es independiente
del \textbf{pasado} y s√≥lo se ve afectado por el \textbf{presente}, es
decir:

\[
\mathbb P[S_{t+1}| S_t] = \mathbb P[ S_{t+1}| S_t, s_{t-1}, ... S_2, S_1]
\]

Tendremos una \textbf{Matriz de Probabilidades de Transici√≥n} a una
matriz con las probabilidades de todos los posibles cambios de estado
que se puedan producir,
\[\mathcal P_{ss'}=\mathbb P[S_{t+1}=s'|S_t = s]\]

\hypertarget{proceso-de-markov}{%
\subsubsection{Proceso de Markov}\label{proceso-de-markov}}

As√≠ llamaremos \textbf{Proceso de Markov} a un proceso aleatorio sin
memmoria, es decir, una secuencia de estados \(S_1, S_2, ‚Ä¶\) con la
propiedad de Markov.

Un Proceso de Markov est√° formado por una dupla
\(<\mathcal S,\mathcal P>\),:

\begin{itemize}
\tightlist
\item
  \(\mathcal S\) conjunto finito de Estados
\item
  \(\mathcal P\) matriz de probabilidades de transici√≥n
\end{itemize}

\hypertarget{proceso-de-recompensa-de-markov}{%
\subsubsection{Proceso de Recompensa de
Markov}\label{proceso-de-recompensa-de-markov}}

LLamaremos \textbf{Proceso de Recompensa de Markov} (\textbf{MRP,} por
sus siglas en ingles) a una cu√°drupla
\(<\mathcal S,\mathcal P,\mathcal R,\gamma>\), formada por:

\begin{itemize}
\tightlist
\item
  \(\mathcal S\) conjunto finito de Estados
\item
  \(\mathcal P\) matriz de probabilidades de transici√≥n
\item
  \(\mathcal R\) Funci√≥n de recompensa definida como:
  \(\mathcal R_s=E[R_{t+1}|S_t=s]\), donde \(R_{t+1}\)es la recompensa
  obtenida de pasar al estado \(S_{t+1}\) desde el estado \(S_t\)
\item
  \(\gamma\) Factor de descuento, con \(\gamma \in [0,1]\)
\end{itemize}

En este contexto llamaremos \textbf{Saldo (}\(G_t\)\textbf{)} a la suma
de todas las recompensas conseguidas a partir del estado \(s_t\) con el
factor de descuento aplicado.

\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty\gamma^kR_{t+k+1}
\]

El hecho de usar \(\gamma\) (\textbf{factor descuento}), nos permite dar
grandes recompensas lo antes posible, y no dar tanto valor a futuras
recompensas lejanas. Tambi√©n puede haber otras interpretaciones por
ejemplo a nivel econ√≥mico, si la recompensa est√° basado en un dato
monetario real, tendr√≠a sentido que el dinero a futuro tendr√≠a menos
valor. Tambi√©n nos permite asegurar que este valor de \(G_t\) es finito
ya que produce que la serie sea convergente.

Cuando los valores del factor descuento se acercan a \textbf{0}
podr√≠amos decir que nos fijamos s√≥lo en los valores m√°s cercanos de la
recompensa. En cambio cuando los valores se acercan a \textbf{1}
entonces les daremos m√°s peso a los valores m√°s lejanos de la
recompensa.

Una vez definido el Saldo podemos definir la \textbf{Funci√≥n Valor de
Estado} como la funci√≥n que nos da el \textbf{Saldo Esperado} comenzando
por el estado \(s\). Es decir:

\[
V(s) = \mathbb E[G_t|S_t=s]
\]

Esta funci√≥n nos dice c√≥mo de bueno es partir de este estado y
continuar.

\hypertarget{proceso-de-decisiuxf3n-de-markov}{%
\subsubsection{Proceso de Decisi√≥n de
Markov}\label{proceso-de-decisiuxf3n-de-markov}}

Un \textbf{Proceso de Decisi√≥n de Markov (MDP por sus siglas en ingl√©s)}
es un tupla \(<\mathcal S,\mathcal A,\mathcal P,\mathcal R, \gamma >\)
donde:

\begin{itemize}
\item
  \(\mathcal S\) es el conjunto de posibles \textbf{estados}.
\item
  \(\mathcal A\) es el conjunto de posibles \textbf{acciones}.
\item
  \(\mathcal P\) son las \textbf{probabilidades de transici√≥n} de un
  estado a otro en funci√≥n de la acci√≥n realizada. Por cada estado y
  acci√≥n hay una distribuci√≥n de probabilidad para pasar a otro estado.

  \[
  \mathcal P_{ss'}^a=\mathbb P[S_{t+1}=s'|S_t=s,A_t=a]\]
\item
  \(\gamma\) es el conocido como \textbf{factor de descuento} y tendr√°
  un valor entre \([0,1)]\) y nos proporciona cuanto descontamos en las
  recompensas a futuro.
\item
  \(\mathcal R\) es la \textbf{Funci√≥n de recompensa} definida como:
  \(\mathcal R_s^a=E[R\_{t+1}|S_t=s, A_t=a]\), donde \(R\_{t+1}\)es la
  recompensa obtenida de pasar al estado \(S_{t+1}\) desde el estado
\end{itemize}

Adem√°s tenemos que este \textbf{proceso estoc√°stico} cumple la
\textbf{propiedad de Markov} que dice que el futuro es independiente del
pasado dado el presente. En t√©rminos de nuestro problema, podr√≠a decir
que pasar de un estado \(s_t\) al siguiente \(s_{t+1}\) s√≥lo depende de
\(s_t\) y no de los anteriores estados \[
\mathbb P(s_{t+1}|s_t)= \mathbb P(s_{t+1}|s_1,s_2,...,s_t)
\]

Veamos cual es la \textbf{din√°mica} de un MDP:

\begin{itemize}
\tightlist
\item
  Empezamos con un estado \(s_0 \in \mathcal S\)
\item
  Elegimos una acci√≥n \(a_0 \in \mathcal A\) (la pol√≠tica ser√° la que la
  elija)
\item
  Obtenemos una recompensa \(R_1 = R(s_0) = R(s_0, a_0)\)
\item
  Elegimos una acci√≥n \(a_1 \in \mathcal A\)(la pol√≠tica ser√° la que la
  elija)
\item
  Se transiciona aleatoriamente a un estado \(s_1\) en un funci√≥n del
  valor de \(P_{s_0s_1}^{a_1}\)
\item
  Obtenemos una recompensa \(R_2 = R(s_1) = R(s_1, a_1)\)
\item
  Se transiciona a aleatoriamente a un estado \(s_12\) en un funci√≥n del
  valor de \(P_{s_1s_2}^{a_2}\)
\item
  \ldots{}
\item
  Repetimos de forma iterativa este proceso
\end{itemize}

La meta en RL es elegir las \textbf{acciones} adecuadas en el tiempo
para \textbf{maximizar}:
\[\mathbb E[G_t] =\mathbb E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \gamma^3 R(s_3) + ...]\]
Que es conocido como la \textbf{hipotesis de la recompensa}.

Vamos a introducir el t√©rmino de \textbf{pol√≠tica} como una funci√≥n
\(\pi : \mathcal S \rightarrow \mathcal A\) que mapea los estados a las
acciones. Es decir, es la que decide que \textbf{acci√≥n} hay que
\textbf{ejecutar} en funci√≥n de \textbf{cual} es el \textbf{estado} en
el que estamos. Una pol√≠tica podr√≠a ser determin√≠stica o estoc√°stica. \[
a = \pi(s) \\
a = \pi (a|s)=\mathbb P[A=a|S=s]
\]

Una pol√≠tica define cual va a ser el comportamiento de un
\textbf{agente}. En un MDP las pol√≠ticas dependen del estado actual, y
no de la historia de los estados pasados.

Diremos que estamos \textbf{ejecutando una pol√≠tica} \(\pi\) si cuando
estamos en un estado \(s\) aplicamos la acci√≥n \(a=\pi(s)\)

Definiremos:

\[
\mathcal P_{s,s`}^\pi = \sum_{a \in \mathcal A}\pi(a|s)\mathcal P_{s,s`}^a \\
\mathcal R_{s}^\pi = \sum_{a \in \mathcal A}\pi(a|s)\mathcal R_{s}^a 
\]

Tambi√©n definiremos la \textbf{Funci√≥n Valor de Estado} para una
\textbf{pol√≠tica} \(\pi\) a la funci√≥n que nos predice la recompensa a
futuro (el \textbf{saldo esperado}):
\[V^{\pi}(s)=\mathbb E_\pi[G_t|S_t=s]=
\mathbb E_\pi[R(s_t) + \gamma R(s_{t+1}) + \gamma^2 R(s_{t+2}) + \gamma^3 R(s_{t+3}) + ...|s_t=s]\]
Es decir, la esperanza de la suma de las recompensas con factor
descuento suponiendo el comienzo en \(s_t=s\) y tomando las acciones
bajo la pol√≠tica \(\pi\). Nos permite decir c√≥mo de buenos o malos son
los estados.

A√±adiremos el concepto de la \textbf{Funci√≥n Valor de Acci√≥n,} tambi√©n
llamada \textbf{Funci√≥n de Calidad} (por eso se usa la \(Q\)
(Quality)\textbf{,} para una \textbf{pol√≠tica} \(\pi\) a la funci√≥n que
nos predice la recompensa a futuro (el saldo esperado), suponiendo que
se se \textbf{parte de una acci√≥n} \(a\). \[
Q^\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]\\
=\mathbb E_\pi[R(s_t) + \gamma R(s_{t+1}) + \gamma^2 R(s_{t+2}) + \gamma^3 R(s_{t+3}) + ...|s_t=s,A_t=a]
\] La funci√≥n de \textbf{Valor de Estado} puede ser descompuesta en la
\textbf{recompensa inmediata} y el resto de la recompensa: \[
V^\pi(s) = \mathbb E_\pi[R_{t+1}+\gamma V^\pi(S_{t+1)}|S_t=s]
\] y del mismo modo se puede descomponer la funci√≥n \textbf{Valor de
Acci√≥n}: \[
Q^\pi(s,a) = \mathbb E_\pi[R_{t+1}+\gamma Q^\pi(S_{t+1}, A_{t+1})|S_t=s,A_t=a]
\] Luego tenemos \[
V^\pi(s) = \sum_{a\in A}\pi(a|s)Q^\pi(s,a)
\]

y \[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}V^\pi(s')
\]

Llegando a

\[
V^\pi(s) = \sum_{a\in A}\pi(a|s)(R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}V^\pi(s'))
\]

y

\[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}\sum_{a \in \mathcal A} \pi (a|s)Q^\pi(s',a)
\]

Dada una pol√≠tica \(\pi\) su \textbf{funci√≥n valor de estado} asociada
\(V^{\pi}(s)\) cumple la \textbf{Ecuaci√≥n de Bellman}:
\[V^{\pi}(s)=R_s+ + \gamma\sum_{s'\in S}P_{s,\pi(s)}(s')V^{\pi}(s')\] Lo
que nos dice que la \textbf{funci√≥n valor} est√° separada en \textbf{dos
t√©rminos}:

\begin{itemize}
\tightlist
\item
  La recompensa inmediata \(R(s)\)
\item
  La suma de recompensas a futuro con el factor de descuento.
\end{itemize}

Igualmente su \textbf{funci√≥n valor de acci√≥n asociada}
\(Q^\pi(s,a)\)cumple la \textbf{Ecuaci√≥n de Bellman}:

\[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}\sum_{a \in \mathcal A} \pi (a|s)Q^\pi(s',a)
\]

Las \textbf{Ecuaciones de Bellman} permiten garantizar una
\textbf{soluci√≥n √≥ptima} del problema de forma que dada una
\textbf{pol√≠tica √≥ptima} (\(\pi^*\)), adem√°s se cumple:

\[
V^{\pi^*}(s)=V^*(s)=max_\pi V^\pi(s)\\
Q^{\pi^*}(s,a)=Q^*(s,a)=max_\pi Q^\pi(s,a)
\]

Es decir, que las funciones de valor de estado y de acci√≥n √≥ptimas son
las mismas que se general con la \textbf{pol√≠tica √≥ptima}.

Como la meta del RL es encontrar una \textbf{pol√≠tica √≥ptima} \(\pi^*\)
la cual maximize el valor del \textbf{saldo esperado total (desde el
inicio)} \(G_0=\sum_{t=0}^\infty\), es decir, podr√≠amos definir la
pol√≠tica √≥ptima como:

\begin{equation}
\pi^*(a|s)= \left\lbrace
\begin{array}{ll}
1 \text{ si } a=\mathop{\mathrm{argmax}}\limits_{a \in \mathcal A} Q^* (s,a) \\
0 \text{ si cualqier otro caso} 
\end{array}
\right.
\end{equation}

Luego si conocemos \(Q^*(s,a)\) inmediatamente tenemos una
\textbf{pol√≠tica √≥ptima.}

\hypertarget{resoluciuxf3n-de-las-ecuaciones-de-bellman}{%
\subsubsection{Resoluci√≥n de las Ecuaciones de
Bellman}\label{resoluciuxf3n-de-las-ecuaciones-de-bellman}}

Las ecuaciones de Bellman pueden ser usadas para resolver de forma
eficiente \(V^\pi\), especialmente en un \textbf{MDP} de un n√∫mero
finito de estado, escribiendo una ecuaci√≥n \(V^\pi (s)\) por cada
estado.

La mayor√≠a de los algoritmos de RL usan las Ecuaciones de Bellman para
resolver el problema. La forma b√°sica de resolverlo es usando
\textbf{prograci√≥n din√°mica} (PD por sus siglas en ingl√©s), aunque nos
encontramos con muchos problemas para resolverla cuando el n√∫mero de
acciones/estados aumenta. Tambi√©n se usan otras t√©cnicas como los
\textbf{m√©todos de montecarlo} (MMC, por sus siglas en ingl√©s) o los
m√©todos de \textbf{diferencia temporal} (TD, por sus siglas en ingles).

Pasemos a ver una clasificaci√≥n de los tipos de algoritmos para resolver
los problemas de RL.

\hypertarget{taxonomuxeda-de-algoritmos}{%
\subsection{Taxonom√≠a de Algoritmos}\label{taxonomuxeda-de-algoritmos}}

Desde OpenAI
(\url{https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\#citations-below})
obtenemos la siguiente taxonom√≠a de algoritmos de RL que nos servir√°
como gu√≠a para entender como clasificar los algoritmos:

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_algorithms.png}

}

\caption{Esquema Aprendizaje por Refuerzo - Fuente: Propia}

\end{figure}

La primera gran separaci√≥n se hace sobre si los algoritmos siguen un
modelo definido (model-baed) o no (modelo-free).

\textbf{Model-free}

Por otro lado los \textbf{model-free} usan la experiencia para aprender
o una o ambas de dos cantidades m√°s simples (valores estado/acci√≥n o
pol√≠ticas).

Las aproximaciones de estos algorimtos son de tres tipos:

\begin{itemize}
\tightlist
\item
  \textbf{Policy Optimization}
\end{itemize}

El agente aprende directamente la funci√≥n pol√≠tica que mapea el estado a
una acci√≥n. Nos podemos encontrar con dos tipos de pol√≠ticas, las
\textbf{pol√≠ticas deterministicas} (no hay incertidumbre en el mapeo) y
las \textbf{pol√≠ticas estoc√°sticas} (tenemos una distribuci√≥n de
probabilidad en las acciones)\textbf{.} En este √∫ltimo caso diremos que
tenemos un Proceso de Decisi√≥n de Markov Parcialmente Observable (POMDP,
por sus siglas en ingles).

\begin{itemize}
\tightlist
\item
  \textbf{Q-Learning}
\end{itemize}

En este caso el agente aprende una funci√≥n valor de acci√≥n \(Q(s,a)\)
que nos dir√° c√≥mo de bueno es tomar una acci√≥n dependiendo del estado.

\begin{itemize}
\tightlist
\item
  \textbf{H√≠bridos}
\end{itemize}

Estos m√©todos combinan la fortaleza de los dos m√©todos anteriores,
aprendiendo tanto la funci√≥n pol√≠tica como la funci√≥n valor de acci√≥n.

\textbf{Model-based}

Los algoritmos \textbf{model-based} usan la experiencia para construir
un modelo interno de transiciones y resultados inmediatos en el entorno.
Las acciones son elegidas mediante b√∫squeda o planificaci√≥n en este
modelo construido.

Las aproximaciones de estos algorimtos son de dos tipos:

\begin{itemize}
\tightlist
\item
  Aprender el Modelo
\end{itemize}

Para aprender el modelo se ejecuta una pol√≠tica base,

\begin{itemize}
\tightlist
\item
  Aprender dado el Modelo
\end{itemize}

Nos centraremos en los algoritmos de tipo \textbf{Model-Free} que son
los m√°s utilizados ya que no requieren del modelo. Si se quieren
profundizar en los diferentes algoritmos, se puede consultar las
documentaci√≥n en:
\url{https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\#links-to-algorithms-in-taxonomy}.

Vamos a ver 2 de los algoritmos de tipo \textbf{Model-free} que nos van
a permitir el ver el paso de un algoritmo sin \textbf{Deep Learning} y
otro en el que se aplica \textbf{Deep Learning} para obtener el objetivo
final de tener un \textbf{agente} capaz de aprender por s√≠ solo a
realizar las tareas espec√≠ficas que se tengan que realizar.

\hypertarget{q-learning-value}{%
\subsection{Q-Learning (value)}\label{q-learning-value}}

Q-Learning es un m√©todo basado en valor y que usa el \textbf{sistema TD}
(actualizaci√≥n su funci√≥n valor en cada paso) para el entrenamiento y su
funci√≥n de valor de estado.

En nombre de \textbf{Q} viende de \textbf{Quality} (calidad), por que
nos da la calidad de la acci√≥n en un determinado estado. Lo que tenemos
es que vamos a tener una \textbf{funci√≥n de valor de acci√≥n (Q-funci√≥n)}
que nos da un valor num√©rico de c√≥mo de buena es a partir de un estado
\textbf{s} y una acci√≥n \textbf{a}.

En este caso tenemos que internamente nuestra \textbf{Q-funci√≥n
(}\(Q(s,a)\)\textbf{)} es una \textbf{Q-tabla}, de forma que cada fila
corresponde a un estado, y cada columna a una de las posibles acciones.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_qlearning.png}

}

\caption{Q-Learning - Fuente:
https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/}

\end{figure}

Es decir, esta tabla va a contener la informaci√≥n de \textbf{recompensa
total esperada} para cada valor de estado y acci√≥n. Cuando nosotros
realizamos el \textbf{entrenamiento} de la Q-funci√≥n, nosotros
conseguimos una funci√≥n que \textbf{optimice} esta \textbf{Q-tabla}.

Si nosotros tenemos una Q-funci√≥n √≥ptima (\(Q^*(s,a)\)), entonces
podremos obtener la \textbf{pol√≠tica √≥ptima} a partir de ella:

\[
\pi^*(s) = \mathop{\mathrm{argmax}}\limits_{a \in \mathcal A}Q^*(s,a)
\] Veamos cuales ser√≠an los pasos que deber√≠amos dar:

\textbf{Inicializamos} nuestra \textbf{Q-Tabla} con valores a 0.
Conforme avanece nuestro entrenamiento estos valores ir√°n cambiando en
funci√≥n de los datos que se obtengan al \textbf{porbar} a realizar
\textbf{acciones} y obtener las \textbf{recompensas} correspondientes.

El siguiente elemento que necesitamos es una \textbf{pol√≠tica de
entrenamiento} (funci√≥n que nos permita elegir que acci√≥n tomar en
funci√≥n del estado en el que estemos), en este caso nuestra pol√≠tica
estar√° basada en los valores de la \textbf{Q-tabla}, es lo que
llamaremos \textbf{explotaci√≥n} (explotamos la informaci√≥n que tenemos
cogiendo la acci√≥n con mejor valor Q) o elegiremos otra acci√≥n, es lo
que llamaremos \textbf{exploraci√≥n} (exploramos nuevos caminos cogiendo
una acci√≥n de forma aleatoria).

Esto es lo que se llama una pol√≠tica \(\epsilon\)-greedy, ya que se usa
un par√°metro \(\epsilon\), valor entre 0 y 1, que nos permite decidir si
elegimos \textbf{explorar} o si queremos \textbf{explotar} los datos que
ya tenemos.

XXXXX Imagen del gr√°fico epsilon (epsilon respecto al n√∫mero de
epsisodios)

Tendremos que:

\begin{itemize}
\tightlist
\item
  con probabilidad 1-\(\epsilon\) nosotros haremos \textbf{explotaci√≥n}
  y
\item
  con probabilidad \(\epsilon\) nosotros haremos \textbf{exploraci√≥n}.
\end{itemize}

Es decir, inicialmente le damos valor 1 a \(\epsilon\) de forma que
empezaremos haciendo \textbf{exploraci√≥n} e iremos bajando este valor de
epsilon conforme avance el entrenamiento para que cada vez usemos m√°s la
\textbf{explotaci√≥n}.

La idea base es que al principio del entrenamiento, lo prioritario es
\textbf{explorar}, es decir, seleccionar una acci√≥n al azar y obtener su
recompensa, ya que nuestra \textbf{Q-Tabla} est√° inicializada a 0.
Conforme avance el entrenamiento nos tendremos que ir fiando m√°s de los
datos que ya tenemos y tendr√° que primar la \textbf{explotaci√≥n} de
nuestros datos de la \textbf{Q-Tabla}. Para hacer √©sto de una forma
efectiva, usaremos un par√°metro \textbf{decay\_epsion} que conforme
avancemos en entrenamiento se encargar√° de ir reduciendo el valor de
\(\epsilon\) para conseguir este efecto.

Una vez que tenemos nuestros elementos base, pasaremos al
\textbf{entrenamiento}, de forma que para todos los \textbf{episodios}
(iteraciones de partidas) que definamos haremos lo siguiente:

\begin{itemize}
\tightlist
\item
  Partimos de un \textbf{estado inicial}, y obtenemos una
  \textbf{acci√≥n} a partir de nuestra \textbf{pol√≠tica de entrenamiento}
\item
  Actualizmos \(\epsilon\) con el nuevo valor en este episodio
\item
  Iteramos para un n√∫mero m√°ximo de pasos dentro de este episodio
\item
  Obtenemos el nuevo estado, as√≠ como la recompensa obtenida
\item
  Actulizamos el valor de la \textbf{Q-Tabla} correspondiente seg√∫n la
  f√≥rmula basada en los m√©todos de \textbf{TD} (Diferencias temporales)
  \[
  Q(s,a) = Q(s,a) + \alpha(R(s,a)+\gamma argmax_aQ(s',a) - Q(s,a))\\
  \text{donde }s\text{ es el estado actual y }s'\text{ es el nuevo estado}
  \]
\item
  Verificamos si se ha llegado al final del juego para salir de este
  espisodio si es el caso
\item
  Cambiamos el \textbf{estado} como el \textbf{nuevo estado}
\end{itemize}

Una vez acabemos nuestro entrenamiento, obtendremos nuestra
\textbf{pol√≠tica √≥ptima} como: \[
\pi^*(s) = \mathop{\mathrm{argmax}}\limits_{a \in \mathcal A}Q^*(s,a)
\]

\textbf{Pseudo c√≥digo Q-Learning}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_algoritmo_qlearning.png}

}

\caption{Algoritmo Q-Learning - Fuente: Propia}

\end{figure}

\hypertarget{dqn-deep-q-learning}{%
\subsection{DQN (Deep Q-Learning)}\label{dqn-deep-q-learning}}

\url{https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/}

Hemos visto el algoritmo de \textbf{Q-Learning} en el que us√°bamos una
\textbf{Q-Tabla}, es decir una tabla donde guard√°bamos todos los valores
de la funci√≥n \(Q(s,a)\) y que entrenando el agente, √©ramos capaces de
conseguir aproximar a la funci√≥n \textbf{Q √≥ptima}, con lo cual ten√≠amos
una \textbf{Pol√≠tica √ìptima}.

Este tipo de algoritmos son v√°lidos cuando nos encontramos con un n√∫mero
``limitado'' de estados y acciones, de forma que la tabla es
relativamente manejable y somos capaces de entrenarla. Si nos
encontramos ante un problema en el que tenemos miles o cientos de miles
de estados no va a ser efectivo construir una tabla y entrenarla para
todas las posibles combinaciones \textbf{etado-acci√≥n}. Para abordar
este tipo de problemas, la mejor soluci√≥n es buscar un
\textbf{aproximador} de la funci√≥n \(Q(s,a)\), que nos permita obtener
la mejor soluci√≥n sin necesidad de entrenar todas las posibles
combinaciones.

Para realizar este trabajo una de las posibles opciones es usar
\textbf{redes neuronales} como funci√≥n aproximadora y que nos abrir√° la
posibilidad de trabajar con problemas en los que existan grandes
cantidades de estados/acciones.

Fue el equipo de \textbf{Deepmind} en 2013
(\url{https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf}), en su art√≠tulo
\textbf{``Human-level control through deep reinforcementlearning''}, los
primeros que decidieron atacar los problemas de alta dimensionalidad de
\textbf{estados/acciones} mediante el uso de \textbf{Redes Neuronales
Profundas}. La forma de probar su c√≥digo fue mediante la implementaci√≥n
de \textbf{agentes} que fueran capaces de aprender a jugar a los
cl√°sicos \textbf{juegos de Atari 2600}. De forma que el agente,
recibiendo la informaci√≥n de entrada de los pixels que hay en cada
momento en pantalla y el marcador del juego, eran capaces de sobrepasar
el rendimiento de algoritmos actuales que hac√≠an ese trabajo. En estte
caso usaron la misma red neuronal, con la misma arquitectura e
hiperpar√°metros para los 49 juegos con los que se probaron.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_qlearning.png}

}

\caption{Deep Q-Learning - Fuente:
https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/}

\end{figure}

Con nuestro algoritmo de \textbf{Q-Learning} ten√≠amos una funci√≥n
\(Q(s,a)\) que implement√°bamos con un tabla y nos daba para cada
\textbf{estado} y cada \textbf{acci√≥n} cual era el valor de \textbf{Q}
(Quality) de la recompensa esperada. Ahora, con \textbf{Deep Q-Learning}
nos encontramos que vamos a tener una red neuronal que ser√° la encargada
de para cada \textbf{estado} obtener el valor de \textbf{Q} para cada
posible \textbf{acci√≥n}.

\textbf{DQN (Deep Q-Network) Arquitectura}

Para poder implementar nuestro trabajo con redes neuronales nos vamos a
encontrar con el problema de entrenar la red neuronal (obtener los
pesos) que permitan alcancar nuestra funci√≥n \textbf{Q-√ìptima} que nos
dar√≠a la \textbf{Pol√≠tica √íptima} que es lo que realmente buscamos.

B√°sicamente para realizar el trabajo usaremos 2 redes neuronales que
tendr√°n la misma arquitectura de forma que el entrenamiento sea estable.

\begin{itemize}
\tightlist
\item
  \textbf{DQN} que ser√° la red de predicci√≥n, y que ser√° la que
  entrenaremos para minimizar el valor del error
  \((R+\gamma argmax_{a'}Q(s',a',w')-Q(s,a,w))^2\)
\item
  \textbf{DQN\_Target} que ser√° la red que calcular√°
  \(R+\gamma argmax_a'Q(s',a',w')\)
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_arquitectura_redes.png}

}

\caption{Arquitectura Redes DQN - Fuente:
https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/}

\end{figure}

\textbf{Experience Replay}

El mecanismo del \textbf{Experience Replay} nos va a permitir entrenar
nuestra red \textbf{DQN} con minibatchs que vamos a extraer de forma
aleatoria de la memoria en la que vamos a ir guardando los resultados
que vamos obteniendo \textbf{\textless s,a,r,s'\textgreater{}}.

√âsto nos va a permitir por un lado \textbf{entrenar} nuestra \textbf{red
de predicci√≥n} y adem√°s va a servirnos para evitar
\textbf{correlaciones} de secuencias consecutivas que pudieran producir
un sesgo en nuestros resultados. De esta manera, al elegir al azar los
elementos que vamos a usar para entrenar la red, no tendr√°n ninguna
relaci√≥n con los datos consecutivos que se van produciendo en los pasos
de los episodios.

\textbf{Algoritmo Deep Q-Learning}

\begin{itemize}
\tightlist
\item
  Obtenemos los datos de entrada, que es el estado.
\item
  Seleccionamos la acci√≥n usando nuestra pol√≠tica de entrenamiento
  epsilon-greedy
\item
  Ejecutamos la acci√≥n y obtenemos el siguiente estado as√≠ como la
  recompensa obtenida
\item
  Almacenamos en memoria \textless s,a,r,s'\textgreater{}
\item
  Si tenemos bastantes elementos en la memoria

  \begin{itemize}
  \tightlist
  \item
    Hacemos un minibatch aleatorio y enteramos la red siendo
    \(R+\gamma argmax_{a'}Q(s',a',w')\) el \textbf{target de la red} y
    \(Q(s,a,w)\) el valor predicho.
  \item
    La funci√≥n de p√©rdida ser√° la de Diferencia de Cuadrados
    \(L = (R+\gamma argmax_a'Q(s',a',w')-Q(s,a,w))^2\)
  \end{itemize}
\item
  Despu√©s de cada C iteraciones, copiaremos los pesos de la red DQN a la
  DQN\_Target
\item
  Repetiremos estos pasos durante M episodios
\end{itemize}

\textbf{Pseudo-c√≥digo Deep Q-Learning}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{imagenes/reinforcement_learning/rl_algoritmo_dqn.png}

}

\caption{Algoritmo Deep Q-Learning - Fuente: Propia}

\end{figure}

\textbf{Variantes de Deep Q-Learning}

\begin{itemize}
\tightlist
\item
  Double Deep Q Network (DDQN) -- 2015
\item
  Deep Recurrent Q Network (DRQN) -- 2015
\item
  Dueling Q Network -- 2015
\item
  Persistent Advantage Learning (PAL) -- 2015
\item
  Bootstrapped Deep Q Network -- 2016
\item
  Normalized Advantage Functions (NAF) = Continuous DQN -- 2016
\item
  N-Step Q Learning -- 2016
\item
  Noisy Deep Q Network (NoisyNet DQN) -- 2017
\item
  Deep Q Learning for Demonstration (DqfD) -- 2017
\item
  Categorical Deep Q Network = Distributed Deep Q Network = C51 -- 2017

  \begin{itemize}
  \tightlist
  \item
    Rainbow -- 2017
  \end{itemize}
\item
  Quantile Regression Deep Q Network (QR-DQN) -- 2017
\item
  Implicit Quantile Network -- 2018
\end{itemize}

\hypertarget{listado-algoritmos}{%
\subsection{Listado Algoritmos}\label{listado-algoritmos}}

\textbf{1. Model-Free}

\textbf{Value-based}

\href{https://link.springer.com/content/pdf/10.1007/BF00992698.pdf}{Q-learning
= SARSA max} -- 1992

\href{http://mi.eng.cam.ac.uk/reports/svr-ftp/auto-pdf/rummery_tr166.pdf}{State
Action Reward State-Action (SARSA)}-- 1994

\href{https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf}{Deep Q Network
(DQN)} -- 2013

\href{https://arxiv.org/pdf/1509.06461.pdf}{Double Deep Q Network
(DDQN)} -- 2015

\href{https://arxiv.org/abs/1507.06527}{Deep Recurrent Q Network (DRQN)}
-- 2015

\href{https://arxiv.org/abs/1511.06581}{Dueling Q Network} -- 2015

\href{https://arxiv.org/abs/1512.04860}{Persistent Advantage Learning
(PAL)} -- 2015

\href{https://arxiv.org/abs/1602.04621}{Bootstrapped Deep Q Network} --
2016

\href{https://arxiv.org/abs/1603.00748}{Normalized Advantage Functions
(NAF) = Continuous DQN} -- 2016

\href{https://arxiv.org/abs/1602.01783}{N-Step Q Learning} -- 2016

\href{https://arxiv.org/abs/1706.10295}{Noisy Deep Q Network (NoisyNet
DQN)} -- 2017

\href{https://arxiv.org/abs/1704.03732}{Deep Q Learning for
Demonstration (DqfD)} -- 2017

\href{https://arxiv.org/abs/1707.06887}{Categorical Deep Q Network =
Distributed Deep Q Network = C51} -- 2017

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1710.02298}{Rainbow} -- 2017
\end{itemize}

\href{https://arxiv.org/pdf/1710.10044v1.pdf}{Quantile Regression Deep Q
Network (QR-DQN)} -- 2017

\href{https://arxiv.org/abs/1806.06923}{Implicit Quantile Network}--
2018

\href{https://arxiv.org/abs/1703.01310}{Mixed Monte Carlo (MMC)} -- 2017

\href{https://arxiv.org/abs/1703.01988}{Neural Episodic Control (NEC)}
-- 2017

\textbf{Policy-based}

\href{https://link.springer.com/article/10.1023/A:1010091220143}{Cross-Entropy
Method (CEM)}-- 1999

Policy Gradient

\begin{itemize}
\tightlist
\item
  \href{https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf}{REINFORCE
  = Vanilla Policy Gradient}(VPG)- 1992
\item
  Policy gradient softmax
\item
  \href{https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf}{Natural
  Policy Gradient (Optimisation) (NPG) / (NPO)} -- 2002
\item
  \href{https://arxiv.org/abs/1604.06778}{Truncated Natural Policy
  Gradient (TNPG)} -- 2016
\end{itemize}

\textbf{Actor-Critic}

\href{https://arxiv.org/abs/1602.01783}{Advantage Actor Critic (A2C)} --
2016

\href{https://arxiv.org/abs/1602.01783}{Asynchronous Advantage
Actor-Critic (A3C)}~ -- 2016

\href{https://arxiv.org/abs/1506.02438}{Generalized Advantage Estimation
(GAE)} -- 2015

\href{https://arxiv.org/abs/1502.05477}{Trust Region Policy Optimization
(TRPO)} -- 2015

\href{http://proceedings.mlr.press/v32/silver14.pdf}{Deterministic
Policy Gradient (DPG)} -- 2014

\href{https://arxiv.org/abs/1509.02971}{Deep Deterministic Policy
Gradients (DDPG)}~ -- 2015

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1804.08617}{Distributed Distributional
  Deterministic Policy Gradients (D4PG)} -- 2018
\item
  \href{https://arxiv.org/pdf/1802.09477.pdf}{Twin Delayed Deep
  Deterministic Policy Gradient (TD3)} -- 2018
\end{itemize}

\href{https://arxiv.org/abs/1611.01224}{Actor-Critic with Experience
Replay (ACER)} -- 2016

\href{https://arxiv.org/abs/1708.05144}{Actor Critic using
Kronecker-Factored Trust Region (ACKTR)} -- 2017

\href{https://arxiv.org/abs/1707.06347}{Proximal Policy Optimization
(PPO)~}-- 2017

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1707.02286}{Distributed PPO (DPPO)} --
  2017
\item
  \href{https://arxiv.org/pdf/1707.06347.pdf}{Clipped PPO (CPPO)}~ --
  2017
\item
  \href{https://arxiv.org/abs/1911.00357}{Decentralized Distributed PPO
  (DD-PPO)}-- 2019
\end{itemize}

\href{https://arxiv.org/abs/1801.01290}{Soft Actor-Critic (SAC)}~ --
2018

\textbf{General Agents}

\begin{itemize}
\tightlist
\item
  \href{https://ieeexplore.ieee.org/document/542381}{Covariance Matrix
  Adaptation Evolution Strategy (CMA-ES)}-- 1996
\item
  \href{https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf}{Episodic
  Reward-Weighted Regression (ERWR)} -- 2009
\item
  \href{https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264}{Relative
  Entropy Policy Search (REPS)}-- 2010
\item
  \href{https://arxiv.org/abs/1611.01779}{Direct Future Prediction
  (DFP)} -- 2016
\end{itemize}

\textbf{Imitation Learning Agents}

Behavioral Cloning (BC)

\href{https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf}{Dataset
Aggregation (Dagger)} (i.e.~query the expert) -- 2011

Adversarial Reinforcement Learning

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1606.03476}{Generative Adversarial
  Imitation Learning (GAIL)} -- 2016
\item
  \href{https://arxiv.org/abs/1710.11248}{Adverserial Inverse
  Reinforcement Learning (AIRL)}-- 2017
\end{itemize}

\href{https://arxiv.org/abs/1710.02410}{Conditional Imitation Learning}
-- 2017

\href{https://arxiv.org/abs/1905.11108}{Soft Q-Imitation Learning
(SQIL)} -- 2019

\textbf{Hierarchical Reinforcement Learning Agents}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1712.00948.pdf}{Hierarchical Actor Critic
  (HAC)} -- 2017
\end{itemize}

\textbf{Memory Types}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1511.05952}{Prioritized Experience Replay
  (PER)} -- 2015
\item
  \href{https://arxiv.org/abs/1707.01495.pdf}{Hindsight Experience
  Replay (HER)} -- 2017
\end{itemize}

\textbf{Exploration Techniques}

\begin{itemize}
\tightlist
\item
  E-Greedy
\item
  Boltzmann
\item
  Ornstein--Uhlenbeck process
\item
  Normal Noise
\item
  Truncated Normal Noise
\item
  \href{https://arxiv.org/abs/1602.04621}{Bootstrapped Deep Q Network}~
\item
  \href{https://arxiv.org/abs/1706.01502}{UCB Exploration via
  Q-Ensembles (UCB)}~
\item
  \href{https://arxiv.org/abs/1706.10295}{Noisy Networks for
  Exploration}~
\item
  \href{https://pathak22.github.io/noreward-rl/}{Intrinsic Curiosity
  Module (ICM)} -- 2017
\end{itemize}

\textbf{Meta Learning}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1703.03400}{Model-agnostic meta-learning
  (MAML)}-- 2017
\item
  \href{https://openreview.net/pdf?id=S1evHerYPr}{Improving
  Generalization in Meta Reinforcement Learning using Learned
  Objectives} (MetaGenRLis) -- 2020
\end{itemize}

\textbf{2. Model-Based}

\textbf{Dyna-Style Algorithms / Model-based data generation}

\begin{itemize}
\tightlist
\item
  \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362\&rep=rep1\&type=pdf}{Dynamic
  Programming (DP) = DYNA-Q} -- 1990
\item
  \href{https://arxiv.org/abs/1506.07365}{Embed to Control (E2C)}-- 2015
\item
  \href{https://arxiv.org/abs/1802.10592}{Model-Ensemble Trust-Region
  Policy Optimization (ME-TRPO)} -- 2018
\item
  \href{https://arxiv.org/abs/1807.03858}{Stochastic Lower Bound
  Optimization (SLBO)} -- 2018
\item
  \href{https://arxiv.org/abs/1809.05214}{Model-Based
  Meta-Policy-Optimzation (MB-MPO)} (meta learning) -- 2018
\item
  \href{https://arxiv.org/abs/1803.00101}{Stochastic Ensemble Value
  Expansion (STEVE)} -- 2018
\item
  \href{https://arxiv.org/abs/1803.00101}{Model-based Value Expansion
  (MVE)} -- 2018
\item
  \href{https://arxiv.org/abs/1903.00374}{Simulated Policy Learning
  (SimPLe)} -- 2019
\item
  \href{https://arxiv.org/abs/1906.08253}{Model Based Policy
  Optimization (MBPO)} -- 2019
\end{itemize}

\textbf{Policy Search with Backpropagation through Time / Analytic
gradient computation}

\begin{itemize}
\tightlist
\item
  \href{https://www.jstor.org/stable/3613752?origin=crossref\&seq=1}{Differential
  Dynamic Programming (DDP)} -- 1970
\item
  \href{http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF}{Linear
  Dynamical Systems and Quadratic Cost (LQR)} -- 1989
\item
  \href{https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf}{Iterative
  Linear Quadratic Regulator (ILQR)} -- 2004
\item
  \href{https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf}{Probabilistic
  Inference for Learning Control (PILCO)} -- 2011
\item
  \href{https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf}{Iterative
  Linear Quadratic-Gaussian (iLQG)} -- 2012
\item
  \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.716.4271\&rep=rep1\&type=pdf}{Approximate
  iterative LQR with Gaussian Processes (AGP-iLQR)} -- 2014
\item
  \href{https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf}{Guided
  Policy Search (GPS)} -- 2013
\item
  \href{https://arxiv.org/abs/1510.09142}{Stochastic Value Gradients
  (SVG)} -- 2015
\item
  \href{https://dl.acm.org/doi/10.5555/3306127.3331874}{Policy search
  with Gaussian Process} -- 2019
\end{itemize}

\textbf{Shooting Algorithms / sampling-based planning}

\href{https://arxiv.org/pdf/1708.02596.pdf}{Random Shooting (RS)} --
2017

\href{https://www.sciencedirect.com/science/article/pii/B9780444538598000035}{Cross-Entropy
Method (CEM)}-- 2013

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1811.04551}{Deep Planning Network
  (DPN)}-2018
\item
  \href{https://arxiv.org/abs/1805.12114}{Probabilistic Ensembles with
  Trajectory Sampling (PETS-RS and PETS-CEM)} -- 2018
\item
  \href{https://arxiv.org/abs/1610.00696}{Visual Foresight} -- 2016
\end{itemize}

\href{https://arxiv.org/abs/1509.01149}{Model Predictive Path Integral
(MPPI)} -- 2015

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1909.11652}{Planning with Deep Dynamics
  Models (PDDM)} -- 2019
\end{itemize}

\href{https://hal.inria.fr/inria-00116992/document}{Monte-Carlo Tree
Search (MCTS)} -- 2006

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1712.01815}{AlphaZero} -- 2017
\end{itemize}

~

\hypertarget{redes-generativas-adversarias}{%
\section{Redes Generativas
Adversarias}\label{redes-generativas-adversarias}}

\hypertarget{actualidad-y-algunos-conceptos-relacionados-con-el-deep-learning}{%
\section{Actualidad y algunos conceptos relacionados con el Deep
Learning}\label{actualidad-y-algunos-conceptos-relacionados-con-el-deep-learning}}

\hypertarget{software-para-aplicar-deep-learning}{%
\section{Software para aplicar Deep
Learning}\label{software-para-aplicar-deep-learning}}

\bookmarksetup{startatroot}

\hypertarget{anuxe1lisis-causal-y-modelos-gruxe1ficos-probabiluxedsticos}{%
\chapter{An√°lisis Causal y Modelos Gr√°ficos
Probabil√≠sticos}\label{anuxe1lisis-causal-y-modelos-gruxe1ficos-probabiluxedsticos}}

\hypertarget{relaciones-y-modelos-causales}{%
\section{Relaciones y Modelos
Causales}\label{relaciones-y-modelos-causales}}

\hypertarget{redes-bayesianas-aprendizaje-y-clasificadores}{%
\section{Redes bayesianas: aprendizaje y
clasificadores}\label{redes-bayesianas-aprendizaje-y-clasificadores}}

\hypertarget{hipuxf3tesis-map-y-naive-bayes}{%
\subsection{Hip√≥tesis MAP y
Naive-Bayes}\label{hipuxf3tesis-map-y-naive-bayes}}

\hypertarget{modelos-bayesianos}{%
\subsection{Modelos bayesianos}\label{modelos-bayesianos}}

\hypertarget{modelos-ocultos-de-markov}{%
\section{Modelos Ocultos de Markov}\label{modelos-ocultos-de-markov}}

\hypertarget{tipos-de-cadenas-de-markov}{%
\subsection{Tipos de cadenas de
Markov}\label{tipos-de-cadenas-de-markov}}

\hypertarget{aplicaciones-de-moms}{%
\subsection{Aplicaciones de MOMs}\label{aplicaciones-de-moms}}

\hypertarget{causal-machine-learning}{%
\section{Causal Machine Learning}\label{causal-machine-learning}}

\hypertarget{efectos-causales.-ate-y-cate}{%
\subsection{Efectos causales. ATE y
CATE}\label{efectos-causales.-ate-y-cate}}

\hypertarget{uplift}{%
\subsection{Uplift}\label{uplift}}

\bookmarksetup{startatroot}

\hypertarget{algoritmos-genuxe9ticos}{%
\chapter{Algoritmos Gen√©ticos}\label{algoritmos-genuxe9ticos}}

Buenas referencias
https://repository.urosario.edu.co/server/api/core/bitstreams/7ae959ec-81de-435b-aca4-e9bb365e4894/content

Buena documentaci√≥n (graficos)

https://www.cs.us.es/\textasciitilde fsancho/Blog/posts/Algoritmos\_Geneticos.md.html

http://www.robolabo.etsit.upm.es/asignaturas/irin/transparencias/AG.pdf

http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/temageneticos.pdf

https://sci2s.ugr.es/sites/default/files/files/Teaching/GraduatesCourses/Bioinformatica/Tema\%2006\%20-\%20AGs\%20I.pdf

Individuo o Cromosoma Compuesto de Genes Codificaci√≥n de genes (Binaria,
Entera, Real)

\hypertarget{introducciuxf3n-4}{%
\section{Introducci√≥n}\label{introducciuxf3n-4}}

Los algoritmos gen√©ticos es un m√©todo bastante com√∫n en miner√≠a de
datos. Se inspiran en el \textbf{proceso natural de selecci√≥n y
evoluci√≥n} tal y como se describe por la teor√≠a evolucionista de la
selecci√≥n natural postulada por \textbf{Darwin}.

Los \textbf{principios} sobre los que se asientan los algoritmos
gen√©ticos son:

\begin{itemize}
\item
  Los individuos \textbf{mejor adaptados} al entorno son aquellos que
  tienen una probabilidad mayor de \textbf{sobrevivir} y, por ende, de
  \textbf{reproducirse}.
\item
  Los descendientes \textbf{heredan} caracter√≠sticas de sus
  progenitores.
\item
  De forma espor√°dica y natural se producen \textbf{mutaciones} en el
  material gen√©tico de algunos individuos, provocando cambios
  permanentes.
\end{itemize}

Los algoritmos gen√©ticos son adecuados para obtener buenas
aproximaciones en \textbf{problemas de b√∫squeda, aprendizaje y
optimizaci√≥n} {[}Marczyk. 2004{]}.

De forma esquem√°tica un algoritmo gen√©tico es una \textbf{funci√≥n
matem√°tica} que tomando como entrada unos individuos iniciales
(\textbf{poblaci√≥n origen}) selecciona aquellos \textbf{ejemplares}
(tambi√©n llamados individuos o cromosomas) que \textbf{recombin√°ndose}
por alg√∫n m√©todo generar√°n como resultado la \textbf{siguiente
generaci√≥n}. Esta funci√≥n se aplicar√° de forma \textbf{iterativa} hasta
verificar alguna condici√≥n de parada, bien pueda ser un n√∫mero m√°ximo de
iteraciones o bien la obtenci√≥n de un individuo que cumpla unas
restricciones iniciales.

\textbf{Condiciones para la aplicaci√≥n de los Algoritmos Gen√©ticos}

No es posible la aplicaci√≥n en toda clase de problemas de los algoritmos
gen√©ticos. Para que estos puedan aplicarse, los problemas deben cumplir
las siguientes condiciones:

\begin{itemize}
\item
  El \textbf{espacio de b√∫squeda} \footnote{\textbf{Recordemos que
    cualquier m√©todo de Data Mining se puede asimilar como una b√∫squeda
    en el espacio soluci√≥n, es decir, el espacio formado por todas las
    posibles soluciones de un problema}} debe estar acotado, por tanto
  ser \textbf{finito}.
\item
  Es necesario poseer una \textbf{funci√≥n} de aptitud, que denominaremos
  \textbf{fitness}, que eval√∫e cada soluci√≥n (individuo) indic√°ndonos de
  forma cuantitativa cu√°n buena o mala es una soluci√≥n concreta.
\item
  Las \textbf{soluciones} deben ser \textbf{codificables} en un lenguaje
  comprensible para un \textbf{ordenador}, y si es posible de la forma
  m√°s \textbf{compacta} y abreviada posible.
\end{itemize}

Habitualmente, la segunda condici√≥n es la m√°s complicada de conseguir,
para ciertos problemas es trivial la funci√≥n de fitness (por ejemplo, en
el caso de la b√∫squeda del m√°ximo de una funci√≥n) no obstante, en la
vida real a veces es muy complicada de obtener y, habitualmente, se
realizan conjeturas evalu√°ndose los algoritmos con varias funciones de
fitness.

\textbf{Ventajas e inconvenientes}

\textbf{Ventajas}

\begin{itemize}
\item
  No necesitan ning√∫n conocimiento particular del problema sobre el que
  trabajan, √∫nicamente cada ejemplar debe representar una posible
  soluci√≥n al problema.
\item
  Es un algoritmo admisible, es decir, con un n√∫mero de iteraciones
  suficiente son capaces de obtener la soluci√≥n √≥ptima en problemas de
  optimizaci√≥n.
\item
  Los algoritmos gen√©ticos son bastante robustos frente a falsas
  soluciones ya que al realizar una inspecci√≥n del espacio soluci√≥n de
  forma no lineal (por ejemplo, si quisi√©ramos obtener el m√°ximo
  absoluto de una funci√≥n) el algoritmo no recorre la funci√≥n de forma
  consecutiva por lo que no se ve afectada por m√°ximos locales.
\item
  Altamente paralelizable, es decir, ya que el c√°lculo no es lineal
  podemos utilizar varias m√°quinas para ejecutar el programa y evaluar
  as√≠ un mayor n√∫mero de casos.
\item
  Pueden ser incrustrables en muchos algoritmos de data mining para
  formar modelos h√≠bridos. Por ejemplo para seleccionar el n√∫mero √≥ptimo
  de neuronas en un modelo de Perceptr√≥n Multicapa.
\end{itemize}

\textbf{Inconvenientes}

\begin{itemize}
\item
  Su coste computacional puede llegar a ser muy elevado, si el espacio
  de trabajo es muy grande.
\item
  En el caso de que no se haga un correcto ajuste de los par√°metros
  pueden llegar a caer en una situaci√≥n de dominaci√≥n en la que se
  produce un bucle infinito ya que unos individuos dominan sobre los
  dem√°s impidiendo la evoluci√≥n de la poblaci√≥n y por tanto inhiben la
  diversidad biol√≥gica.
\item
  Puede llegar a ser muy complicado encontrar una funci√≥n de evaluaci√≥n
  de cada uno de los individuos para seleccionar los mejores de los
  peores.
\end{itemize}

\hypertarget{fundamentos-teuxf3ricos-conceptos}{%
\section{Fundamentos te√≥ricos
(conceptos)}\label{fundamentos-teuxf3ricos-conceptos}}

A continuaci√≥n, se explican, someramente, los conceptos b√°sicos de los
algoritmos gen√©ticos.

\hypertarget{codificaciuxf3n-de-los-datos}{%
\subsection{Codificaci√≥n de los
datos}\label{codificaciuxf3n-de-los-datos}}

Cada \textbf{individuo o cromosoma} est√° formado por unos cuantos
\textbf{genes}. Estos individuos con sus genes los tenemos que
representar de forma que podamos codificar esa informaci√≥n.

Los principales m√©todos de representaci√≥n son: - \textbf{Binaria:} Los
individuos/cromosomas est√°n representados por una serie de genes que son
bits ( valores 0 √≥ 1).

\begin{itemize}
\item
  \textbf{Entera:} Los individuos/cromosomas est√°n representados por una
  serie de genes que son n√∫meros enteros.
\item
  \textbf{Real:} Los individuos/cromosomas est√°n representados por una
  serie de genes que son n√∫meros reales en coma flotante.
\item
  \textbf{Permutacional:} Los individuos/cromosomas est√°n representados
  por una serie de genes que son permutaciones de un conjunto de
  elementos. Se usan en aquellos problemas en los que la secuencia u
  orden es importante.
\item
  \textbf{Basada en √°rboles:} Los individuos/cromosomas est√°n
  representados por una serie de genes que son estructuras jer√°rquicas.
\end{itemize}

El primer paso para conseguir que un ordenador procese unos
\textbf{datos} es conseguir \textbf{representarlos} de una forma
apropiada. En primer t√©rmino, para codificar los datos, es necesario
separar las posibles configuraciones posibles del dominio del problema
en un \textbf{conjunto} de \textbf{estados} \textbf{finito}.

Una vez obtenida esta clasificaci√≥n el objetivo es representar cada
\textbf{estado} de \textbf{forma} \textbf{un√≠voca} con una cadena de
caracteres (compuesta en la mayor√≠a de casos por unos y ceros).

A pesar de que cada estado puede codificarse con alfabetos de diferente
cardinalidad\footnote{La longitud de las cadenas que representen los
  posibles estados no es necesario que sea fija, representaciones como
  la de Kitano para representar operaciones matem√°ticas son un ejemplo
  de esto}, uno de los resultados fundamentales de la teor√≠a de
algoritmos gen√©ticos es el \textbf{teorema del esquema}, que afirma que
la codificaci√≥n \textbf{√≥ptima} es aquella en la que los algoritmos
tienen un alfabeto de cardinalidad, es decir el uso del
\textbf{alfabeto} \textbf{binario}.

El enunciado del \textbf{teorema} del \textbf{esquema} es el siguiente:
¬´\emph{Esquemas cortos, de bajo orden y aptitud superior al promedio
reciben un incremento exponencial de representantes en generaciones
subsecuentes de un Algoritmo Gen√©tico.}¬ª

Una de las ventajas de usar un alfabeto binario para la construcci√≥n de
configuraciones de estados es la sencillez de los operadores utilizados
para la modificaci√≥n de estas. En el caso de que el alfabeto sea
binario, los operadores se denominan, l√≥gicamente, \textbf{operadores}
\textbf{binarios}. Es importante destacar que variables que est√©n
pr√≥ximas en el espacio del problema deben preferiblemente estarlo en la
codificaci√≥n ya que la proximidad entre ellas condiciona un elemento
determinante en la mutaci√≥n y reproducibilidad de √©stas. Es decir, dos
estados que en nuestro espacio de estados del universo del problema que
est√°n consecutivos deber√≠an estarlo en la representaci√≥n de los datos,
esto es √∫til para que cuando haya mutaciones los saltos se den entre
estados consecutivos. En t√©rminos generales cumplir esta premisa mejora
experimentalmente los resultados obtenidos con algoritmos gen√©ticos.

En la pr√°ctica el factor que condiciona en mayor grado el fracaso o el
\textbf{√©xito} de la aplicaci√≥n de algoritmos gen√©ticos a un problema
dado es una \textbf{codificaci√≥n} \textbf{acorde} con los
\textbf{datos}.

Otra opci√≥n muy com√∫n es establecer a cada uno de los posibles casos un
\textbf{n√∫mero} \textbf{natural} y luego codificar ese n√∫mero en binario
natural, de esta forma minimizamos el problema que surge al concatenar
m√∫ltiples variables independientes en el que su representaci√≥n binaria
diera lugar a numerosos huecos que produjeran soluciones no v√°lidas. Por
ejemplo, tenemos 3 variables, las dos primeras tienen 3 posibles estados
y la √∫ltima dos, el n√∫mero posible de estados es 3+3+2 = 8, combinando
las 3 variables podemos codificar todo con 3 bits en comparaci√≥n con los
2+2+1 = 5 bits necesarios que utilizar√≠amos en el caso de realizar el
procedimiento anterior. En este ejemplo no s√≥lo ahorrar√≠amos espacio
sino que adem√°s evitar√≠amos que se produjeran individuos cuya soluci√≥n
no es factible.

\hypertarget{algoritmo}{%
\subsection{Algoritmo}\label{algoritmo}}

Un algoritmo gen√©tico implementado en \textbf{pseudo c√≥digo} podr√≠a ser
el siguiente:

\texttt{Generar\ de\ forma\ aleatoria\ una\ poblaci√≥n\ inicial\ aleatoria.\ Cada\ individuo/cromosoma\ tiene\ sus\ propios\ genes.}

\texttt{Mientras\ (condici√≥n\ de\ terminaci√≥n\ es\ falsa).}

\texttt{Evaluar\ el\ fitness\ de\ cada\ uno\ de\ los\ individuos.}

\texttt{Selecci√≥n\ de\ los\ individuos\ con\ mejor\ fitness.}

\texttt{Recombinaci√≥n\ de\ los\ individuos.}

\texttt{Mutaci√≥n\ de\ los\ indiviudos.}

Un posible esquema que puede representar una posible implementaci√≥n de
algoritmos gen√©ticos se muestra en la figura
Figure~\ref{fig-esquema-genetico} .

\begin{figure}

{\centering \includegraphics{imagenes/capitulo3/esquema_genetico.png}

}

\caption{\label{fig-esquema-genetico}Esquema de implementaci√≥n de un
algoritmo gen√©tico}

\end{figure}

A continuaci√≥n, en los siguientes apartados, se har√° una descripci√≥n de
las fases anteriormente expuestas:

\textbf{Inicializar Poblaci√≥n}

Como ya se ha explicado antes el primer paso es inicializar la poblaci√≥n
origen. Habitualmente la inicializaci√≥n se hace de forma
\textbf{aleatoria} procurando una \textbf{distribuci√≥n}
\textbf{homog√©nea} en los casos iniciales de prueba. No obstante, si se
tiene un conocimiento m√°s profundo del problema es posible obtener
mejores resultados inicializando la poblaci√≥n de una forma apropiada a
la clase de soluciones que se esperan obtener.

\textbf{Evaluar Poblaci√≥n}

Durante cada \textbf{iteraci√≥n} (generaci√≥n) cada gen se decodifica
convirti√©ndose en un grupo de par√°metros del problema y se eval√∫a el
problema con esos datos.

Pongamos por ejemplo que queremos evaluar el m√°ximo de la funci√≥n
\(f(x)=x¬≤\) en el intervalo \([0,1]\) y supongamos que construimos cada
gen con \textbf{6 d√≠gitos} \((2^6=64)\) , por lo que interpretando el
n√∫mero obtenido en binario natural y dividi√©ndolo entre 64 obtendremos
el punto de la funci√≥n que corresponde al gen (individuo). Evaluando
dicho punto en la funci√≥n que queremos evaluar (\(f(x)=x¬≤\)) obtenemos
lo que en nuestro caso ser√≠a el \textbf{fitness}, en este caso cuanto
mayor fitness tenga un gen mejor valorado est√° y m√°s probable es que
prospere su descendencia en el futuro. No en todas las implementaciones
de algoritmos gen√©ticos se realiza una fase de evaluaci√≥n de la
poblaci√≥n tal y como aqu√≠ est√° descrita, en ciertas ocasiones se omite y
no se genera ning√∫n fitness asociado a cada estado evaluado. Selecci√≥n
La fase de selecci√≥n elije los individuos a reproducirse en la pr√≥xima
generaci√≥n, esta selecci√≥n puede realizarse por muy distintos m√©todos.

En el algoritmo mostrado en pseudo c√≥digo anteriormente el
\textbf{m√©todo} de \textbf{selecci√≥n} usado depende del fitness de cada
individuo. A continuaci√≥n, se describen los m√°s comunes:

\textbf{Selecci√≥n elitista:} Se seleccionan los individuos con mayor
fitness de cada generaci√≥n. La mayor√≠a de los algoritmos gen√©ticos no
aplican un elitismo puro, sino que en cada generaci√≥n eval√∫an el fitness
de cada uno de los individuos, en el caso de que los mejores de la
anterior generaci√≥n sean mejores que los de la actual √©stos se copian
sin recombinaci√≥n a la siguiente generaci√≥n.

\textbf{Selecci√≥n proporcional a la aptitud:} los individuos m√°s aptos
tienen m√°s probabilidad de ser seleccionados, asign√°ndoles una
probabilidad de selecci√≥n m√°s alta. Una vez seleccionadas las
probabilidades de selecci√≥n a cada uno de los individuos se genera una
nueva poblaci√≥n teniendo en cuenta √©stas.

\textbf{Selecci√≥n por rueda de ruleta:} Es un m√©todo conceptualmente
similar al anterior. Se le asigna una probabilidad absoluta de aparici√≥n
de cada individuo de acuerdo al fitness de forma que ocupe un tramo del
intervalo total de probabilidad (de 0 a 1) de forma acorde a su fitness.
Una vez completado el tramo total se generan n√∫meros aleatorios de 0 a 1
de forma que se seleccionen los individuos que ser√°n el caldo de cultivo
de la siguiente generaci√≥n.

\textbf{Selecci√≥n por torneo:} se eligen subgrupos de individuos de la
poblaci√≥n, y los miembros de cada subgrupo compiten entre ellos. S√≥lo se
elige a un individuo de cada subgrupo para la reproducci√≥n.

\textbf{Selecci√≥n por rango:} a cada individuo de la poblaci√≥n se le
asigna un rango num√©rico basado en su fitness, y la selecci√≥n se basa en
este ranking, en lugar de las diferencias absolutas en el fitness. La
ventaja de este m√©todo es que puede evitar que individuos muy aptos
ganen dominancia al principio a expensas de los menos aptos, lo que
reducir√≠a la diversidad gen√©tica de la poblaci√≥n y podr√≠a obstaculizar
la b√∫squeda de una soluci√≥n aceptable.

Un ejemplo de esto podr√≠a ser que al intentar maximizar una funci√≥n el
algoritmo gen√©tico convergiera hac√≠a un m√°ximo local que posee un
fitness mucho mejor que el de sus cong√©neres de poblaci√≥n lo que har√≠a
que hubiera una dominancia clara con la consecuente desaparici√≥n de los
individuos menos aptos (con peor fitness).

\textbf{Selecci√≥n generacional}: la descendencia de los individuos
seleccionados en cada generaci√≥n se convierte en la siguiente
generaci√≥n. No se conservan individuos entre las generaciones.

\textbf{Selecci√≥n por estado estacionario:} la descendencia de los
individuos seleccionados en cada generaci√≥n vuelve al acervo gen√©tico
preexistente, reemplazando a algunos de los miembros menos aptos de la
siguiente generaci√≥n. Se conservan algunos individuos entre
generaciones.

\textbf{B√∫squeda del estado estacionario:} Ordenamos todos los genes por
su fitness en orden decreciente y eliminamos los √∫ltimos m genes, que se
sustituyen por otros m descendientes de los dem√°s. Este m√©todo tiende a
estabilizarse y converger.

\textbf{Selecci√≥n jer√°rquica:} los individuos atraviesan m√∫ltiples
rondas de selecci√≥n en cada generaci√≥n. Las evaluaciones de los primeros
niveles son m√°s r√°pidas y menos discriminatorias, mientras que los que
sobreviven hasta niveles m√°s altos son evaluados m√°s rigurosamente. La
ventaja de este m√©todo es que reduce el tiempo total de c√°lculo al
utilizar una evaluaci√≥n m√°s r√°pida y menos selectiva para eliminar a la
mayor√≠a de los individuos que se muestran poco o nada prometedores, y
sometiendo a una evaluaci√≥n de aptitud m√°s rigurosa y computacionalmente
m√°s costosa s√≥lo a los que sobreviven a esta prueba inicial.

\textbf{Recombinaci√≥n}.

\textbf{Recombinaci√≥n} tambi√©n llamada \textbf{Cross-over o
reproducci√≥n}. La recombinaci√≥n es el operador gen√©tico m√°s utilizado y
consiste en el \textbf{intercambio} de \textbf{material}
\textbf{gen√©tico} entre \textbf{dos} \textbf{individuos} al azar (pueden
ser incluso entre el mismo elemento). El material gen√©tico se
intercambia entre \textbf{bloques}. Gracias a la presi√≥n
selectiva\footnote{Presi√≥n Selectiva es la fuerza a la que se ven
  sometido naturalmente los genes con el paso del tiempo. Con el
  sucesivo paso de las generaciones los genes menos √∫tiles estar√°n
  sometidos a una mayor presi√≥n selectiva produci√©ndose la paulatina
  desaparici√≥n de estos} ir√°n predominando los mejores bloques g√©nicos.

Existen diversos \textbf{tipos} de \textbf{cross-over:}

\textbf{Cross-over de 1 punto.} Los cromosomas se cortan por 1 punto y
se intercambian los dos bloques de genes.

\textbf{Cross-over de n-puntos.} Los cromosomas se cortan por n puntos y
el resultado se intercambia.

\textbf{Cross-over uniforme.} Se genera un patr√≥n aleatorio en binario,
y en los elementos que haya un 1 se realiza intercambio gen√©tico.

\textbf{Cross-over especializados.} En ocasiones, el espacio de
soluciones no es continuo y hay soluciones que a pesar de que sean
factibles de producirse en el gen no lo son en la realidad, por lo que
hay que incluir restricciones al realizar la recombinaci√≥n que impidan
la aparici√≥n de algunas combinaciones.

\textbf{Mutaci√≥n}.

Este fen√≥meno, generalmente muy raro en la naturaleza, se modela de la
siguiente forma: cuando se genera un gen hijo se examinan uno a uno los
bits del mismo y se genera un coeficiente aleatorio para cada uno. En el
caso de que alg√∫n coeficiente supere un cierto umbral se modifica dicho
bit. Modificando el umbral podemos variar la probabilidad de la
mutaci√≥n. Las mutaciones son un mecanismo muy interesante por el cual es
posible generar nuevos individuos con rasgos distintos a sus
predecesores.

Los \textbf{tipos} de \textbf{mutaci√≥n} m√°s conocidos son:

\begin{verbatim}
En la imagen @fig-mutacion mostramos gr√°ficamente este tipo.

![Esquema de mutaci√≥n multibit de un algoritmo gen√©tico](imagenes/capitulo3/mutacion.png){#fig-mutacion}
\end{verbatim}

\begin{itemize}
\item
  \textbf{Mutaci√≥n de gen}\footnote{\textbf{Gen e Individuo en este
    contexto es lo mismo}}: existe una √∫nica probabilidad de que se
  produzca una mutaci√≥n de alg√∫n bit. De producirse, el algoritmo toma
  aleatoriamente un bit, y lo invierte.
\item
  \textbf{Mutaci√≥n multigen:} cada bit tiene una probabilidad de mutarse
  o no, que es calculada en cada pasada del operador de mutaci√≥n
  multibit.
\item
  \textbf{Mutaci√≥n de intercambio:} Se intercambia el contenido de dos
  genes aleatoriamente.
\item
  \textbf{Mutaci√≥n de barajado:} existe una probabilidad de que se
  produzca una mutaci√≥n. De producirse, toma dos genes aleatoriamente y
  baraja de forma aleatoria los genes, seg√∫n hubi√©ramos escogido,
  comprendidos entre los dos.
\end{itemize}

\textbf{Condici√≥n de finalizaci√≥n}

Una vez que se ha generado la nueva poblaci√≥n se eval√∫a la misma y se
selecciona a aquel individuo o aquellos que por su fitness se consideran
los m√°s aptos.

\hypertarget{otros-operadores}{%
\subsection{Otros Operadores}\label{otros-operadores}}

Los operadores descritos anteriormente suelen ser operadores
\textbf{generalistas} (aplicables y de hecho aplicados a todo tipo de
problemas), sin embargo, para ciertos contextos suele ser m√°s
recomendable el uso de operadores espec√≠ficos para realizar un recorrido
por el espacio de soluci√≥n m√°s acorde a la soluci√≥n buscada.

\textbf{Modificadores de la longitud de los individuos}. En ocasiones
las soluciones no son una combinaci√≥n de todas las variables de entrada,
en estas ocasiones los individuos deber√°n tener una longitud
variable\footnote{En muchas ocasiones, se realizan estudios de miner√≠a
  de datos sobre todos los datos existentes, encontr√°ndose en ellos
  variables esp√∫reas, es decir, variables que no aportan nada de
  informaci√≥n para el problema evaluado}. L√≥gicamente, en este tipo de
casos, es necesario modificar la longitud de los individuos, para ello
haremos uso de los operadores a√±adir y quitar, que a√±adir√°n o quitar√°n a
un individuo un trozo de su carga g√©nica (es decir, un trozo de
informaci√≥n).

\hypertarget{paruxe1metros-necesarios-al-aplicar-algoritmos-genuxe9ticos}{%
\subsection{Par√°metros necesarios al aplicar Algoritmos
Gen√©ticos}\label{paruxe1metros-necesarios-al-aplicar-algoritmos-genuxe9ticos}}

Cualquier algoritmo gen√©tico necesita ciertos par√°metros que deben
fijarse antes de cada ejecuci√≥n, como:

\textbf{Tama√±o de la poblaci√≥n:} Determina el tama√±o m√°ximo de la
poblaci√≥n a obtener. En la pr√°ctica debe ser de un valor lo
suficientemente grande para permitir diversidad de soluciones e intentar
llegar a una buena soluci√≥n, pero siendo un n√∫mero que sea computable en
un tiempo razonable.

\textbf{Condici√≥n de terminaci√≥n:} Es la condici√≥n de parada del
algoritmo. Habitualmente es la convergencia de la soluci√≥n (si es que la
hay), un n√∫mero prefijado de generaciones o una aproximaci√≥n a la
soluci√≥n con un cierto margen de error.

\textbf{Individuos que intervienen en la reproducci√≥n de cada
generaci√≥n:} se especifica el porcentaje de individuos de la poblaci√≥n
total que formar√°n parte del acervo de padres de la siguiente
generaci√≥n. Esta proporci√≥n es denominada proporci√≥n de cruces.

\textbf{Probabilidad de ocurrencia de una mutaci√≥n:} En toda ejecuci√≥n
de un algoritmo gen√©tico hay que decidir con qu√© frecuencia se va a
aplicar la mutaci√≥n. Se debe de a√±adir alg√∫n par√°metro adicional que
indique con qu√© frecuencia se va a aplicar dentro de cada gen del
cromosoma. La frecuencia de aplicaci√≥n de cada operador estar√° en
funci√≥n del problema; teniendo en cuenta los efectos de cada operador,
tendr√° que aplicarse con cierta frecuencia o no. Generalmente, la
mutaci√≥n y otros operadores que generen diversidad se suelen aplicar con
poca frecuencia; la recombinaci√≥n se suele aplicar con frecuencia alta.

\hypertarget{conclusiones}{%
\section{Conclusiones}\label{conclusiones}}

Los algoritmos gen√©ticos es uno de los enfoques m√°s originales en data
mining. Su sencillez, combinada con su flexibilidad les proporciona una
robustez que les hace adecuados a infinidad de problemas. No obstante,
su simplicidad y sobre todo independencia del problema hace que sean
algoritmos poco espec√≠ficos. Recorriendo este cap√≠tulo hemos visto los
numerosos par√°metros y m√©todos aplicables a los algoritmos gen√©ticos que
nos ayudan a realizar una adaptaci√≥n de los algoritmos gen√©ticos m√°s
concreta a un problema. En definitiva, la implementaci√≥n de esquemas
evolutivos tal y como se describen en biolog√≠a podemos afirmar que
funciona.

\hypertarget{ejemplos-pruxe1cticos}{%
\section{Ejemplos pr√°cticos}\label{ejemplos-pruxe1cticos}}

\hypertarget{algoritmos-genuxe9ticos-con-r}{%
\subsection{Algoritmos gen√©ticos con
R}\label{algoritmos-genuxe9ticos-con-r}}

\textbf{Selecci√≥n de variables}

El objetivo del ejemplo es ver c√≥mo podemos usar un algoritmo gen√©tico
para hacer una selecci√≥n de variables, qued√°ndonos s√≥lo con unas pocas.

Para ver que estamos acertados en la selecci√≥n de variables vamos a
tomar el ejemplo del \textbf{dataset} de \textbf{Iris}, que es un
problema de \textbf{clasificaci√≥n} con \textbf{3 clases}, cuenta con
\textbf{150 muestras} y \textbf{4 variables} explicativas. Como queremos
usarlo para selecci√≥n de variables lo que vamos a realizar es meter de
forma \textbf{sint√©tica} \textbf{10 variables m√°s}, que siguen una
\textbf{distribuci√≥n normal} (0,1) y veremos el comportamiento del
algoritmo a ver si realmente aparecen las variables originales como
parte de las m√°s importantes.

Tendremos que, para el algoritmo gen√©tico, nuestro cromosoma o individuo
ser√° un \textbf{vector} de tama√±o 14 (\textbf{14 genes}), que representa
las \textbf{14 variables} del dataset que hemos preparado.

En la imagen Figure~\ref{fig-poblacion} mostramosla poblaci√≥n en una
iteraci√≥n N.

\begin{figure}

{\centering \includegraphics{imagenes/capitulo3/poblacion.png}

}

\caption{\label{fig-poblacion}Poblaci√≥n en iteraci√≥n N}

\end{figure}

\textbf{Funci√≥n Fitness} (Evaluaci√≥n)

Cuando estamos trabajando con selecci√≥n de variables, el objetivo es
conseguir el conjunto de variables que mejor modelo construyan seg√∫n
nuestro dataset. En este caso, al ser un problema de
\textbf{clasificaci√≥n}, veremos cual es la combinaci√≥n de variables que
nos da \textbf{menos errores al clasificar}.

Nuestra funci√≥n fitness deber√° seguir estos \textbf{pasos}:

- Recibe una \textbf{variable} llamada \textbf{indices} que tiene el
tama√±o del numero de variables (el tama√±o del cromosoma) que hay en el
dataframe (en nuestro caso 14) de datos.

\begin{itemize}
\tightlist
\item
  Los valores son \textbf{1} si esa variable se va a \textbf{usar} y
  \textbf{0} si \textbf{no} se va a \textbf{usar}.
\end{itemize}

- Se construye un \textbf{modelo}, en este caso usamos \textbf{LDA}
(An√°lisis Discriminante Lineal) con las \textbf{variables} que tienen
\textbf{valor 1}.

- Calculamos el \textbf{error} que queremos minimizar (n√∫mero de fallos)

- Para este caso del LDA cogemos los valores \textbf{\$posterior} que
nos dan la probabilidad de cada clase para cada entrada de la muestra

- Calculamos cual es el \textbf{m√°ximo} y as√≠ le asignamos esa
\textbf{clase} como su soluci√≥n. Tambi√©n podr√≠amos coger directamente el
valor de \$class con la clase dada como predicci√≥n.

- Verificamos cuantos hemos fallado y lo dividimos por el n√∫mero de
muestras para ver el \textbf{porcentaje} de \textbf{fallos}

- Devolvemos el \textbf{porcentaje de fallos}. El resultado de la
ejecuci√≥n del algoritmo evolutivo (rbga.bin())) nos dar√° un objeto del
que tendremos que obtener que variables son las que queremos usar.

Figure~\ref{fig-poblacion_final}

\begin{figure}

{\centering \includegraphics{imagenes/capitulo3/poblacion_final.png}

}

\caption{\label{fig-poblacion_final}Poblaci√≥n Final}

\end{figure}

Una vez que nuestro algoritmo pare, deber√≠amos tener la poblaci√≥n que
mejor se ha adaptado seg√∫n el fitness que hab√≠amos definido.

En nuestro caso estar√°n las 100 mejores combinaciones de variables, que
dan el menor error al clasificar. De esta manera si para cada variable
contamos cuantas veces ha salido en cada elemento de la poblaci√≥n,
sabremos cuantas veces se ha usado en las combinaciones de variables de
esta √∫ltima iteraci√≥n (que es la mejor hasta ese momento). Con lo cual
podremos saber cuales han sido las \textbf{variables} \textbf{m√°s}
\textbf{usadas} en la poblaci√≥n final.

El objeto \textbf{modelo\_evolutivo}, que nos devuelve rbga.bin, tiene
una variable \textbf{population} de dimensiones tama√±o\_poblaci√≥n x
numero\_variables , en nuestro caso de 100x14, que tiene la informaci√≥n
de la poblaci√≥n de la \textbf{√∫ltima iteraci√≥n del algoritmo}, que en un
principio deber√≠a ser la mejor. Este population contiene para cada fila
(elemento en la poblaci√≥n), 0 o 1 en la posici√≥n que corresponde a cada
variable.

Para ver cuales son las variables que m√°s se han usado tenemos que sumar
por columnas y ese dato nos dar√° para cada columna (corresponde con una
variable) la cantidad de veces que se ha usado en esta poblaci√≥n. Una
vez tenemos estos datos ya podemos quedarnos con el n√∫mero de variables
que deseemos cogiendo las que m√°s alto valor tienen.

Algunos de los \textbf{par√°metros} que tenemos que definir y que algunos
pueden afectar al rendimiento:

\begin{itemize}
\item
  \textbf{size:} N√∫mero de genes, en nuestro caso, n√∫mero de variables
\item
  \textbf{popsize:} Tama√±o de la poblaci√≥n con la que se trabaja en cada
  iteraci√≥n. Cuanto m√°s alto sea este valor, m√°s tiempo tardar√° la
  ejecuci√≥n. En este caso vamos a usar 100 elementos en la poblaci√≥n, es
  decir, en cada iteraci√≥n habr√° 100 combinaciones de variables.
\item
  \textbf{iters:} N√∫mero de iteraciones del algoritmo. Va a depender del
  problema pero seguramente entre 50 y 100 tendr√≠amos una buena
  soluci√≥n. Cuanto mayor sea esta variable, mayor ser√° el tiempo de
  ejecuci√≥n.
\item
  \textbf{zeroToOneRatio:} Con esta variable le indicamos al algoritmo
  cual es el ratio de ceros frente a unos cuando se generan elementos de
  la poblaci√≥n, es decir, m√°s o menos nos tiene que dar una idea de
  cuantas variables se van a usar a la vez (las que tienen unos).
  Podr√≠amos estimar que si tenemos 14 variables y queremos reducir a
  tener alrededor de 4, deber√≠amos intentar en cada iteraci√≥n tener un
  rango parecido a ese. La forma de conseguirlo ser√≠a poner un valor a
  esta variable de 1, es decir, cada 1 cero hay 1 uno. Lo que m√°s o
  menos nos dejar√≠a alrededor de 7 variables con 1. Como esto se genera
  de forma aleatoria, m√°s o menos estaremos en un rango de variables
  √∫til parecido a lo que queremos.
\item
  \textbf{verbose:} Esta variable si la ponemos a TRUE se encargar√° de
  sacarnos informaci√≥n de la evoluci√≥n del algoritmo. La recomendaci√≥n
  es dejarla a FALSE ya que incrementa mucho el tiempo de ejecuci√≥n.
\end{itemize}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
```r \\
 \\
``` \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{library}\NormalTok{(genalg) }\FunctionTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{data}\NormalTok{(iris) }\FunctionTok{set.seed}\NormalTok{(}\DecValTok{999}\NormalTok{) }\CommentTok{\# Alas variables reales del dataset, les a√±adimos 10 variables m√°s ficiticas (normal 0,1) \# Ponemos estas variables al principio X \textless{}{-} cbind(scale(iris[,1:4]),matrix(rnorm(10*150), 150, 10)) Y \textless{}{-} iris[,5]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{iris.evaluate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(indices) \{}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{result }\OtherTok{=} \DecValTok{1} \CommentTok{\# Tiene que haber al menos 2 variables if (sum(indices) \textgreater{} 2) \{}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\CommentTok{\# Creamos un modelo de clasificaci√≥n con LDA usando s√≥lo las variables que vienen}
\CommentTok{\# marcadas en la variablindices con valor 1}
\CommentTok{\# El LDA tiene el valor $posterior que devuelve la probabilidad de cada clase (tenemos tres clases en Y)}
\CommentTok{\# Podr√≠amos usar el valor de $class y as√≠ tendr√≠amos directamente la clase.}
\NormalTok{modelo\_lda }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(X[,indices}\SpecialCharTok{==}\DecValTok{1}\NormalTok{], Y, }\AttributeTok{CV=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{$}\NormalTok{posterior}

\CommentTok{\# Cogemos la probabilidad m√°s alta apply(modelo\_lda, 1,function(x)which(x == max(x))), para cada fila (150 muestras)}
\CommentTok{\# Comprobamos cuantos hemos fallado y lo dividimos por el tama√±o de Y (150 muestras)}
\CommentTok{\# El objetivo es que sea m√≠nimo este n√∫mero de fallos}
\NormalTok{result }\OtherTok{=} \FunctionTok{sum}\NormalTok{(Y }\SpecialCharTok{!=} \FunctionTok{dimnames}\NormalTok{(modelo\_lda)[[}\DecValTok{2}\NormalTok{]][}\FunctionTok{apply}\NormalTok{(modelo\_lda, }\DecValTok{1}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x)}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(x)))]) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(Y)}\ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{result }\ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Ejecutamos el algoritmo evolutivo}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{system.time}\NormalTok{(\{ modelo\_evolutivo }\OtherTok{\textless{}{-}} \FunctionTok{rbga.bin}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{, }\AttributeTok{mutationChance=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{zeroToOneRatio=}\DecValTok{1}\NormalTok{,}\AttributeTok{evalFunc=}\NormalTok{iris.evaluate, }\AttributeTok{verbose=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{iters =} \DecValTok{50}\NormalTok{, }\AttributeTok{popSize =} \DecValTok{100}\NormalTok{) \}) }\DocumentationTok{\#\# user system elapsed \#\# 17.05 0.72 22.03}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Veamos como ha quedado el resultado de la poblaci√≥n seg√∫nla gr√°fica que nos da cuantas veces aparece cada variable en la poblaci√≥n final. }\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Mostramos cual es el que ha aparecido m√°s veces en la √∫ltima iteraci√≥n}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{uso\_variables }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(modelo\_evolutivo}\SpecialCharTok{$}\NormalTok{population)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Visualizamos cuanto se ha usado cada variable en esta √∫ltima poblaci√≥n}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Construimos un dataframe con los datos}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{datos }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{variables=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{),}\AttributeTok{uso=}\NormalTok{uso\_variables)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{ggplot}\NormalTok{(datos,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{variables,}\AttributeTok{y=}\NormalTok{uso, }\AttributeTok{fill=}\NormalTok{uso)) }\SpecialCharTok{+} \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Obtenemos ahora cuales son las variables con los valores m√°s altos. Primero vemos las suma de las columnas, que nos dar√° los valores de cuantas veces aparece cada variable, y luego hacemos una funci√≥n que nos dice cuales son las variables m√°s usadas pas√°ndole como par√°metro el vector de uso\_variables y luego cuantas variables queremos quedarnos. }\CommentTok{\# Creamos una funci√≥n que nos da de un vector los √≠ndices de posici√≥n \# donde est√°n las X variables m√°s usadas \# Le pasamos como par√°metro el vector y cuantas variables queremos que devuelva}
\end{Highlighting}
\end{Shaded}

Figure~\ref{fig-frecuencia_variables}

Imagen \ref{fig-frecuencia_variables}

\begin{figure}

{\centering \includegraphics{imagenes/capitulo3/frecuencia_variables.png}

}

\caption{\label{fig-frecuencia_variables}Frecuencia de las Variables}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{posicion\_maximos }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(datos, cuantos) \{ variables }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \ControlFlowTok{if}\NormalTok{( cuantos}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{) \{ }\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{cuantos ) \{ variables[i] }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(datos) datos[variables[i]] }\OtherTok{\textless{}{-}} \DecValTok{0}\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ErrorTok{\}}\NormalTok{ variables}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{Obtenemos ahora cuales son las }\DecValTok{6}\NormalTok{ variables m√°s usados}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{posicion\_maximos}\NormalTok{(uso\_variables, }\DecValTok{6}\NormalTok{) }\DocumentationTok{\#\# [1] 2 4 1 3 8 13 }
\end{Highlighting}
\end{Shaded}

\hypertarget{algoritmos-genuxe9ticos-en-python}{%
\subsection{Algoritmos gen√©ticos en
Python}\label{algoritmos-genuxe9ticos-en-python}}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ImportTok{import}\NormalTok{ os }\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd }\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np }\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt }\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\ImportTok{from}\NormalTok{ genetic\_selection }\ImportTok{import}\NormalTok{ GeneticSelectionCV}

\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler }\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split }\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\textbf{Cargar datos de trabajo}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]

\NormalTok{os.chdir(}\StringTok{\textquotesingle{}C:/Users/p\_san/Desktop/M√°ster\_2020/M√≥dulo\_5\textquotesingle{}}\NormalTok{) }\CommentTok{\#directorio datos=pd.read\_csv(\textquotesingle{}german\_credit.csv\textquotesingle{},encoding = \textquotesingle{}ISO{-}8859{-}1\textquotesingle{}, index\_col=None)}
\end{Highlighting}
\end{Shaded}

Todas las variables son categ√≥ricas salvo:

duration

credit\_amount

residence\_since

age

existing\_credits

num\_dependents

Conversi√≥n a variables categ√≥ricas

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}checking\_status\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}checking\_status\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}credit\_history\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}credit\_history\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}purpose\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}purpose\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}savings\_status\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}savings\_status\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}employment\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}employment\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}personal\_status\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}personal\_status\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}other\_parties\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}other\_parties\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}property\_magnitude\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}property\_magnitude\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}other\_payment\_plans\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}other\_payment\_plans\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}housing\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}housing\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}job\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}job\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}property\_magnitude\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}property\_magnitude\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}own\_telephone\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}own\_telephone\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}foreign\_worker\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}foreign\_worker\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{datos\textbackslash{}[}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{\textbackslash{}].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La variable class es una variable reservada en diferentes m√≥dulos de
Python -\textgreater{} reemplazar por por target

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{datos.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{\}, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{) datos\textbackslash{}[}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{=}\NormalTok{np.where(datos\textbackslash{}[}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{\textbackslash{}]}\OperatorTok{==}\StringTok{\textquotesingle{}good\textquotesingle{}}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) \textbackslash{}}\CommentTok{\# cambio en la codificaci√≥n por sencillez en el preprocesado}
\end{Highlighting}
\end{Shaded}

Definici√≥n de la muestra de trabajo

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{datos\_entrada}\OperatorTok{=}\NormalTok{datos.drop(}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) \textbackslash{}}\CommentTok{\# Datos de entrada datos\_entrada= pd.get\_dummies(datos\_entrada, drop\_first=True) \#conversi√≥n a variables dummy}
\end{Highlighting}
\end{Shaded}

datos de salida

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{respuesta}\OperatorTok{=}\NormalTok{datos.loc\textbackslash{}[:, }\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{\textbackslash{}]}
\end{Highlighting}
\end{Shaded}

Escalado de las variables, partici√≥n de la muestra y Cross Validation

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{seed}\OperatorTok{=}\DecValTok{123}\NormalTok{ \textbackslash{}}\CommentTok{\# Escalado de los datos de entrada x\_esc=StandardScaler().fit\_transform(datos\_entrada) x\_esc=pd.DataFrame(x\_esc, columns=datos\_entrada.columns)}
\end{Highlighting}
\end{Shaded}

Partici√≥n de la muestra

test\_size=0.3 \#muestra para el test x\_train, x\_test, y\_train,
y\_test = train\_test\_split(x\_esc,respuesta, test\_size=test\_size,
random\_state=seed, stratify=respuesta) Usando un modelo Cart from
sklearn.tree import DecisionTreeClassifier
cart=DecisionTreeClassifier(max\_depth=5, random\_state=seed)
cart\_algoritmo\_gen=GeneticSelectionCV(cart, cv=5, \# 5 particiones
verbose=0, \# no se muestran los resultados en la pantalla
scoring=``roc\_auc'', \# ejemplo m√©trica para evaluar max\_features=15,
\# n√∫mero de variables m√°ximas en la selecci√≥n de\\
\# caracter√≠sticas n\_population=50, \# tama√±o de la poblaci√≥n
crossover\_proba=0.5, \# probabilidad de cruce entre parejas de genes
mutation\_proba=0.2, \#probabilidad de mutaci√≥n n\_generations=40,
\#n√∫mero de generaciones crossover\_independent\_proba=0.5, \# prob.
cruce para genes \# independientes mutation\_independent\_proba=0.05, \#
prob. mutaci√≥n de genes \# independientes tournament\_size=3, \#tama√±o
de los grupos n\_gen\_no\_change=10, \# genes que se mantienen
-\textgreater{} no pasan a \# la segunda generaci√≥n caching=True,
n\_jobs=1)

cart\_algoritmo\_gen=cart\_algoritmo\_gen.fit(x\_train, y\_train)\\
ajuste del modelo usando algoritmo gen√©ticos para la selecci√≥n de
variables \# Variables Seleccionadas print(`Num. Var:',
cart\_algoritmo\_gen.n\_features\_) \#
print(cart\_algoritmo\_gen.support\_)\\
\# Resultados matriz numpy -\textgreater{} mala visualizaci√≥n. Los
resultados se \# convierten a df de pandas
df=pd.DataFrame(cart\_algoritmo\_gen.support\_,
columns={[}`Variables'{]}, index=x\_train.columns)
df=df.loc{[}\textasciitilde df{[}`Variables'{]}.isin({[}False{]}){]}
\#se elimina del df las variables no seleccionadas por el algoritmo
list(df.index) \# variables seleccionadas por el algoritmo

Num. Var: 9 {[}`checking\_status\_0\textless=X\textless200',
`checking\_status\_\textless0', ``credit\_history\_`critical/other
existing credit'\,'', ``purpose\_`used car'\,'',
`savings\_status\_\textgreater=1000', ``personal\_status\_`male
single'\,'', `other\_parties\_guarantor',
`other\_payment\_plans\_stores', `job\_skilled'{]}

Resultados test - predicci√≥n \& Matriz de confusi√≥n (modelo CART con
selecci√≥n de variables a trav√©s de Algoritmos Gen√©ticos)

pred=cart\_algoritmo\_gen.predict(x\_test)

confusion\_matrix(y\_test, pred) \# Matriz de confusi√≥n

array({[}{[}173, 37{]}, {[} 46, 44{]}{]}, dtype=int64)

\bookmarksetup{startatroot}

\hypertarget{luxf3gica-difusa}{%
\chapter{L√≥gica Difusa}\label{luxf3gica-difusa}}

\hypertarget{conceptos-clave.}{%
\section{Conceptos clave.}\label{conceptos-clave.}}

\hypertarget{conjuntos-difusos-y-funciones-de-membresuxeda}{%
\subsection{Conjuntos difusos y funciones de
membres√≠a}\label{conjuntos-difusos-y-funciones-de-membresuxeda}}

\hypertarget{inferencia-y-modelamiento-difuso}{%
\subsection{Inferencia y Modelamiento
difuso}\label{inferencia-y-modelamiento-difuso}}

\hypertarget{ejemplos-pruxe1cticos-1}{%
\section{Ejemplos pr√°cticos}\label{ejemplos-pruxe1cticos-1}}

\hypertarget{clustering-difuso}{%
\subsection{Clustering Difuso}\label{clustering-difuso}}

\hypertarget{herramientas-de-diagnuxf3stico}{%
\subsection{Herramientas de
Diagn√≥stico}\label{herramientas-de-diagnuxf3stico}}

\bookmarksetup{startatroot}

\hypertarget{bibliografia}{%
\chapter*{Bibliografia}\label{bibliografia}}
\addcontentsline{toc}{chapter}{Bibliografia}

\markboth{Bibliografia}{Bibliografia}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-knuth84}{}}%
Knuth, Donald E. 1984. {``Literate Programming.''} \emph{Comput. J.} 27
(2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{anexo-1}{%
\chapter{Anexo 1}\label{anexo-1}}

Anexo 1



\printindex

\end{document}
