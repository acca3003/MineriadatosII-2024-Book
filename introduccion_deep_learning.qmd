## Introducción

El **deep learning o aprendizaje profundo** es un concepto amplio, un término de moda que atiende a una realidad que cada día está más presente entre nosotros, y que no tiene una única definición que se pueda considerar veraz, aunque podemos generalizar la definición afirmando que surge de la idea de imitar el cerebro a partir de la utilización de hardware y software, para crear inteligencia artificial. Este intento de imitar al cerebro se ha plasmado en lo que se conoce como redes neuronales artificiales (RNA) que están dotadas de una capacidad de abstracción jerárquica: representan los datos de entrada en varios "niveles", a través de arquitecturas en varias capas, cada capa aprende patrones más complejos según la profundidad de ésta, para conseguir información útil para el aprendizaje.

Podemos afirmar que el principio general del funcionamiento de una arquitectura profunda es guiar el entrenamiento de las capas intermedias utilizando aprendizaje no supervisado. Algunos de los prototipos de arquitecturas y/o algoritmos desarrollados se basan en otras arquitecturas "más simples", modificándolas y obteniendo una arquitectura "más profunda", lo que quiere decir que en sus capas y neuronas se separan las características de un conjunto de datos de entrada de una forma jerarquizada.

Otra definición dada del aprendizaje profundo es que es un subconjunto de los algoritmos usados en machine learning, y se caracterizan por tener una **arquitectura en capas** donde cada capa aprende patrones más complejos según la profundidad de ésta. En este sentido, existen ya muchos prototipos de redes neuronales que tienen esta característica, por ejemplo, las ya muy conocidas redes convolucionales o las redes recurrentes. El Perceptron Multicapa de una sola capa explicado en el módulo 5 no es considerado un método de aprendizaje profundo. El Deep Learning es una técnica reciente donde aún se sigue investigando y desarrollándose y que surgió a partir del año 2006, año en el que se considera fue retomada su investigación. El principal problema por el que se había ralentizado su progreso es que el entrenamiento basado en el método del gradiente para redes neuronales profundas supervisadas, según muchos investigadores, se estancaba en lo que se llama mínimo local aparente, lo que significa que una red con más capas ocultas, es decir una red neuronal más profunda, obtiene peores resultados que una red con sólo una capa oculta. Los investigadores abordaron este tema planteado a través de diferentes ópticas y se descubre que se obtienen resultados más precisos si cada capa de la red es entrenada a través de un algoritmo de preentrenamiento no supervisado (el método del gradiente requiere que sea supervisado) Para el entrenamiento de las capas, diversos autores sugieren utilizar autoencoders o máquinas de Boltzmann y, una vez se hayan preentrenado las diferentes capas, se puede aplicar un criterio supervisado. En estos últimos años se ha producido un rápido crecimiento en la cantidad de arquitecturas y/o algoritmos de entrenamiento en una RNA, incluso, variantes de ellos, las cuales siguen el concepto planteado anteriormente.
